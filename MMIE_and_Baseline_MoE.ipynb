{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eccd1962-1680-4fbc-8bd2-ffbd6fbf750c",
   "metadata": {},
   "source": [
    "# Code File for Baseline MoE and MoE with Monotonic Individual Experts\n",
    "*Produce the figure, monotonicity score, average MSE, and standard error of MSE*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe10f20-5806-4870-a5c3-2a992d9f1204",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "*Make sure you are in the correct coding environment and import dependencies*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db37fa3-fc5c-4f72-8642-921a50043ee7",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!which python3\n",
    "\n",
    "# force reset\n",
    "%reset -f "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7965f121-1008-4171-aa07-4d21e9afd493",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import GPy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from matplotlib.cm import get_cmap\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "\n",
    "import math\n",
    "import numbers\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import numpy.random as npr\n",
    "from scipy.special import logsumexp as lse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from GPy.mappings import Linear\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "import scipy\n",
    "import numpy\n",
    "import time\n",
    "import cvxpy as cp\n",
    "from scipy.stats import t\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import binom\n",
    "from scipy.special import comb\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.interpolate import PchipInterpolator\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import davies_bouldin_score, adjusted_rand_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ccb392-68fc-486a-91fe-a1167f098792",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "*Run the following cells to define them*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6095ef57-62e5-4113-8e4f-686b8cec0dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_turbulence(arr, seed=1):\n",
    "    \"\"\"\n",
    "    Function to add turbulence to duplicate elements\n",
    "    \"\"\"\n",
    "    unique, counts = np.unique(arr, return_counts=True)\n",
    "    duplicates = unique[counts > 1]\n",
    "    \n",
    "    for duplicate in duplicates:\n",
    "        # Find all indices of the duplicate element\n",
    "        indices = np.where(arr == duplicate)[0]\n",
    "        # Add small random noise to all but the first occurrence\n",
    "        for idx in indices[1:]:\n",
    "            np.random.seed(seed) # if this is here, then we are always adding the same number\n",
    "            arr[idx] += np.random.uniform(1e-10, 1e-9)  # small random noise\n",
    "            \n",
    "    return arr\n",
    "\n",
    "def add_turbulence2(arr, seed=1):\n",
    "    \"\"\"\n",
    "    Function to add turbulence to duplicate elements\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    unique, counts = np.unique(arr, return_counts=True)\n",
    "    duplicates = unique[counts > 1]\n",
    "    \n",
    "    for duplicate in duplicates:\n",
    "        # Find all indices of the duplicate element\n",
    "        indices = np.where(arr == duplicate)[0]\n",
    "        # Add small random noise to all but the first occurrence\n",
    "        for idx in indices[1:]: \n",
    "            arr[idx] += np.random.uniform(1e-10, 1e-9)  # small random noise\n",
    "            \n",
    "    return arr\n",
    "\n",
    "def test_add_turbulence():\n",
    "    \"\"\"\n",
    "    Function to test the add_turbulence function\n",
    "    \"\"\"\n",
    "\n",
    "    # Original array\n",
    "    original_array = np.array([1.0, 2.0, 2.0, 3.0, 4.0, 4.0, 4.0, 5.0])\n",
    "    \n",
    "    # Apply turbulence\n",
    "    modified_array = add_turbulence(original_array.copy())\n",
    "    \n",
    "    print(\"Original array:\", original_array)\n",
    "    print(\"Modified array:\", modified_array)\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd90652-063a-4b04-97f8-587f286fda2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class monotonic_poly:\n",
    "\n",
    "    \"\"\"\n",
    "    Class for the BP polynomial model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, Y): \n",
    "\n",
    "        \"\"\"\n",
    "        X: (n, 1)\n",
    "        Y: (n,)\n",
    "        \"\"\"\n",
    "\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.deg = global_degree\n",
    "        self.u_opt = np.zeros(self.deg+1)\n",
    "        self.var = np.eye(self.deg+1) # variance-covariance matrix\n",
    "        self.noise = 1 # will get updated in the function update_variance()\n",
    "\n",
    "    def bernvander(self, x, deg):\n",
    "\n",
    "        \"\"\"\n",
    "        the binomial pmf with \"deg\" trials and the probability given by \"x\"\n",
    "        the berstein basis B(deg,x)\n",
    "        returns a 2D array, a design matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        return binom.pmf(np.arange(1 + deg), deg, x.reshape(-1, 1))\n",
    "\n",
    "    def to_fit(self, indiv_mono_str=\"yes_mono\"):\n",
    "\n",
    "        x = self.X.reshape(-1)\n",
    "        y = self.Y.reshape(-1)\n",
    "                               \n",
    "        deg = global_degree # define number of points and noise \n",
    "        alpha = global_alpha\n",
    "        \n",
    "        u = cp.Variable(deg + 1) # a placeholder for the optimal Bernstein coefficients\n",
    "        loss = cp.sum_squares(self.bernvander(x, deg) @ u - y) # the sum of residual squares\n",
    "        reg = alpha * cp.sum_squares(cp.diff(u, 2))       # penalty for 2nd order differences\n",
    "        if indiv_mono_str == \"yes_mono\":\n",
    "            constraints = [cp.diff(u) >= 0]                   # constraints - u_{i+1} - u_i >= 0\n",
    "            problem = cp.Problem(cp.Minimize(loss + reg), constraints)\n",
    "        else:\n",
    "            problem = cp.Problem(cp.Minimize(loss + reg))\n",
    "        \n",
    "        problem.solve()\n",
    "        #print(f'status: {problem.status}')\n",
    "        u_opt = u.value\n",
    "\n",
    "        # update\n",
    "        self.deg = deg\n",
    "        self.u_opt = u_opt\n",
    "    \n",
    "        return\n",
    "\n",
    "\n",
    "    def to_predict(self, x_test):\n",
    "        \"\"\"\n",
    "        Predict for new x using the current coefficients\n",
    "        x_test: (n, 1)\n",
    "        y_pred: (n,)\n",
    "        \"\"\"\n",
    "\n",
    "        x_test = x_test.reshape(-1)\n",
    "\n",
    "        y_pred = self.bernvander(x_test, self.deg) @ self.u_opt\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "    def to_predict_var(self, x_test):\n",
    "\n",
    "        \"\"\"\n",
    "        Give prediction uncertainty for new x \n",
    "        \"\"\"\n",
    "\n",
    "        x_test = x_test.reshape(-1)\n",
    "\n",
    "        J = self.bernvander(x_test, self.deg)\n",
    "        \n",
    "        y_var = J @ self.var @ J.T + self.noise # add a noise to go from confidence interval to prediction interval\n",
    "\n",
    "        # print(f'are they equal? {np.array_equal(J @ (self.var+self.noise) @ J.T, y_var)}')\n",
    "        # print(f'one mean and std are {np.mean(J @ (self.var+self.noise) @ J.T)} and {np.std(J @ (self.var+self.noise) @ J.T)}')\n",
    "        # print(f'another mean and std are {np.mean(y_var)} and {np.std(y_var)}')\n",
    "        \n",
    "        return y_var\n",
    "\n",
    "    def current_score(self):\n",
    "        \"\"\"\n",
    "        provide a measure of how good the current data fits the current model\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.X.reshape(-1)\n",
    "\n",
    "        temp_pred = self.to_predict(x)\n",
    "        \n",
    "        temp_true = self.Y\n",
    "\n",
    "        MSE = ((temp_true - temp_pred)** 2).mean()\n",
    "\n",
    "        output = 1/(MSE+1e-16)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "def test_poly():\n",
    "\n",
    "    \"\"\"\n",
    "    simply testing the BP poly class\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    x_train = np.sort(np.random.rand(100)).reshape(-1, 1)\n",
    "    y_train = (2 * x_train).ravel() + 0.3 * np.random.randn(100)\n",
    "\n",
    "    x_test = np.linspace(0, 1, 1000).reshape(-1, 1)\n",
    "\n",
    "    model = monotonic_poly(x_train, y_train)\n",
    "    model.to_fit()\n",
    "\n",
    "    y_pred = model.to_predict(x_test)\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(x_train, y_train, label='Training data')\n",
    "    plt.plot(x_test, y_pred, color='red', label='monotonic poly prediction')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend()\n",
    "    plt.title('Monotonic Poly Model')\n",
    "    plt.show()\n",
    "\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de37709a-9b36-427c-8648-8015886696b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_preserve_order(arr):\n",
    "    \"\"\"\n",
    "    grab the unique elements from the array while preserving order\n",
    "    \"\"\"\n",
    "\n",
    "    unique_dict = dict.fromkeys(arr) \n",
    "    # creates a new dictionary where the keys are the elements from arr\n",
    "    # dictionary keys must be unique\n",
    "    # In Python 3.7 and later, dictionaries maintain the order of insertion\n",
    "    \n",
    "    return np.array(list(unique_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331d0a32-c6ca-4dc5-a0e5-a54559b4cc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_variance(z, obsmodel):\n",
    "    \"\"\"\n",
    "    This function updates the variance of a new BP model\n",
    "    \"\"\"\n",
    "    \n",
    "    # creates a new object that is a completely independent copy of the original\n",
    "    z = copy.deepcopy(z)\n",
    "    obsmodel = copy.deepcopy(obsmodel)\n",
    "\n",
    "    unique_z = unique_preserve_order(z[z>=0])\n",
    "\n",
    "    def binomial_prob(deg, i, x):\n",
    "        \"\"\"\n",
    "        bionomial PMF for only i and only 1 x\n",
    "        \"\"\"\n",
    "\n",
    "        output = comb(deg, i) * (x ** i) * ((1 - x) ** (deg - i))\n",
    "            \n",
    "        return output\n",
    "\n",
    "    def obtain_coefficient_var(beta_hat, X, y_data):\n",
    "\n",
    "        \"\"\"\n",
    "        y_data is the true response variable\n",
    "        X is the design matrix\n",
    "        beta_hat is the estimated coefficients\n",
    "        \"\"\"\n",
    "    \n",
    "        residuals = y_data - np.dot(X, beta_hat)\n",
    "        rss = np.sum(residuals**2)\n",
    "        \n",
    "        n = len(y_data)\n",
    "        p = len(beta_hat)\n",
    "        \n",
    "        sigma2_hat = rss / (n - p) # residual variance (n needs to be > p)\n",
    "        matrix = np.dot(X.T, X) + 0.01*np.eye(X.shape[1])\n",
    "\n",
    "        var_beta_hat = sigma2_hat * np.linalg.pinv(matrix) # coefficient uncertainty\n",
    "        # The pseudoinverse is a generalization of the inverse matrix.\n",
    "        # Not all matrices are invertible\n",
    "        # so the pseudoinverse provides a way to obtain a matrix that behaves similarly to an inverse.\n",
    "    \n",
    "        return var_beta_hat, sigma2_hat\n",
    "\n",
    "    for c in np.arange(unique_z.shape[0]): # two\n",
    "    \n",
    "        u = obsmodel[unique_z[c]].u_opt\n",
    "        n = len(u)\n",
    "        deg = n - 1\n",
    "\n",
    "        x = obsmodel[unique_z[c]].X.reshape(-1)\n",
    "        y = obsmodel[unique_z[c]].Y.reshape(-1)\n",
    "\n",
    "        b = np.zeros((len(x), n))\n",
    "        for i in np.arange(b.shape[0]):\n",
    "            for k in np.arange(b.shape[1]):\n",
    "                b[i,k] = binomial_prob(deg, k, x[i])\n",
    "        \n",
    "        var, sigma2_hat = obtain_coefficient_var(beta_hat=u, X=b, y_data=y)\n",
    "        obsmodel[unique_z[c]].var = var\n",
    "        obsmodel[unique_z[c]].noise = sigma2_hat\n",
    "\n",
    "    return obsmodel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992d0e9d-dd84-4de2-8830-5a9df8565fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MOBP:\n",
    "    \"\"\"\n",
    "    Gibbs sampler, return expected cluster assignments and expert parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, X, Y,\n",
    "                 alpha,\n",
    "                 num_init_clusters=2, \n",
    "                 num_iter=100,\n",
    "                 indiv_mono_str=\"yes_mono\"): \n",
    "\n",
    "        self.X = X # (n,1)?\n",
    "        self.Y = Y # (n)?\n",
    "  \n",
    "        self.alpha = alpha\n",
    "        self.Nk = None # the number of points in each cluster (including empty clusters)\n",
    "        self.N, _ = X.shape # the number of data points\n",
    "        \n",
    "        self.num_iter = num_iter\n",
    "        self.num_init_clusters = num_init_clusters\n",
    "\n",
    "        self.z = np.zeros(X.shape[0]) # cluster assignments\n",
    "        self.p = dict()  \n",
    "        self.obsmodel = dict()\n",
    "        self.indiv_mono_str = indiv_mono_str\n",
    "\n",
    "\n",
    "    def initialize_sampler(self, init_K):\n",
    "        \"\"\"\n",
    "        initialize self.z and self.obsmodel\n",
    "        \"\"\"\n",
    "\n",
    "        np.random.seed(0)\n",
    "        x_data = self.X[:, 0].reshape(-1, 1) \n",
    "        \n",
    "        initial_labels_ = np.concatenate([np.ones(global_first_cluster_size, dtype=int),\n",
    "                                          np.zeros(len(x_data)-global_first_cluster_size, dtype=int)])\n",
    "        self.z = initial_labels_\n",
    "        \n",
    "        for k in np.arange(len(np.unique(initial_labels_))):\n",
    "            \n",
    "            Xk = self.X[initial_labels_ == k]\n",
    "            Yk = self.Y[initial_labels_ == k]\n",
    "            self.obsmodel[k] = monotonic_poly(\n",
    "                                     Xk[~np.isnan(Xk)].reshape(-1, 1), \n",
    "                                     Yk[~np.isnan(Yk)].reshape(-1, 1))\n",
    "            self.obsmodel[k].to_fit(indiv_mono_str=self.indiv_mono_str)\n",
    "        \n",
    "        self.obsmodel = update_variance(self.z, self.obsmodel) # for all models\n",
    "\n",
    "\n",
    "    def sample(self):\n",
    "\n",
    "        self.initialize_sampler(init_K=self.num_init_clusters)\n",
    "\n",
    "        self.Nk = np.bincount(self.z) # the number of points in each cluster (including empty clusters)\n",
    "        \n",
    "        idx = np.arange(self.N)\n",
    "        #print('Cluster Initialization: {}'.format(self.Nk[self.Nk > 0])) # the number of points in each cluster (excluding empty clusters)\n",
    "        \n",
    "        for l in np.arange(1, self.num_iter): # (inclusive, exclusive)\n",
    "            for n in np.random.permutation(idx):\n",
    "\n",
    "                \"\"\"------------------- various updates ------------------------\n",
    "                \"\"\"\n",
    "                \n",
    "                curr_k = self.z[n] # save it, as self.z[n] will be changed\n",
    "                if self.Nk[curr_k] == 1: # skip the point that is alone\n",
    "                    continue\n",
    "\n",
    "                # -------- moved here -----------------------                    \n",
    "\n",
    "                # find the clusters that satisfy some conditions (only assign to these clusters)\n",
    "                occupancy = np.hstack([self.Nk, self.alpha])\n",
    "                temp_condition = (occupancy > 0) & (occupancy <= global_ub * self.X.shape[0]) & (occupancy >= global_lb * self.X.shape[0])\n",
    "                active_comp_ids = np.where(temp_condition)[0]\n",
    "                \n",
    "                if len(active_comp_ids) > 0:\n",
    "                    active_comp_ids = np.append(active_comp_ids, len(occupancy)-1) # still consider going to a new cluster\n",
    "\n",
    "                if len(active_comp_ids) == 0: # skip this point if no cluster satisfies these conditions\n",
    "                    continue \n",
    "                # -----------------------------------------------\n",
    "\n",
    "                # remove the data point from the current cluster\n",
    "                self.z[n] = -1 \n",
    "                self.Nk[curr_k] -= 1\n",
    "                \n",
    "                if self.X[self.z == curr_k].shape[0] == 0: # if no point after the removal\n",
    "                    self.obsmodel[curr_k] = None\n",
    "                else: # still some points after the removal\n",
    "                    # update the model\n",
    "                    Xk = self.X[self.z == curr_k]\n",
    "                    Yk = self.Y[self.z == curr_k] \n",
    "                    self.obsmodel[curr_k] = monotonic_poly(\n",
    "                                     Xk[~np.isnan(Xk)].reshape(-1, 1), \n",
    "                                     Yk[~np.isnan(Yk)].reshape(-1, 1))\n",
    "                    self.obsmodel[curr_k].to_fit(indiv_mono_str=self.indiv_mono_str)\n",
    "                    self.obsmodel = update_variance(self.z, self.obsmodel)\n",
    "\n",
    "                # create the new model as if the point will go there\n",
    "                new_comp_model = monotonic_poly(\n",
    "                                     self.X[n, ~np.isnan(self.X[n])].reshape(-1, 1), \n",
    "                                     self.Y[n, ~np.isnan(self.Y[n])].reshape(-1, 1))\n",
    "                new_comp_model.to_fit(indiv_mono_str=self.indiv_mono_str)\n",
    "\n",
    "                \"\"\" ------------------------------ end --------------------------------\"\"\"\n",
    "\n",
    "                \"\"\" --------------- calculate the cluster probability ---------------\"\"\"\n",
    "\n",
    "                def kernel(x, y, gamma=global_gamma):\n",
    "                    return np.exp(-gamma * (x - y)**2)\n",
    "                    \n",
    "                def kernel_derivative(x, y, gamma=global_gamma):\n",
    "                    return -2 * gamma * (x - y) * kernel(x, y, gamma)\n",
    "\n",
    "                def a(x, S, T, gamma=global_gamma): # gating function\n",
    "                    numerator = sum(kernel(x, xi, gamma) for xi in S)\n",
    "                    denominator = sum(kernel(x, xi, gamma) for xi in T)\n",
    "                    return numerator / denominator if denominator != 0 else 0\n",
    "\n",
    "                def derivative_a(x, S, T, gamma=global_gamma): # derivative of gating function\n",
    "                    sum_k_S = np.sum(kernel(x, S, gamma))\n",
    "                    sum_k_T = np.sum(kernel(x, T, gamma))\n",
    "                    sum_k_prime_S = np.sum(kernel_derivative(x, S, gamma))\n",
    "                    sum_k_prime_T = np.sum(kernel_derivative(x, T, gamma))\n",
    "                    numerator = sum_k_prime_S * sum_k_T - sum_k_S * sum_k_prime_T\n",
    "                    denominator = sum_k_T**2\n",
    "                    return numerator / denominator if denominator != 0 else 0\n",
    "\n",
    "                # it returns a array of booleans where each entry is True if the corresponding element is NaN\n",
    "                temp_x = self.X[n, ~np.isnan(self.X[n])].reshape(-1, 1) # current point's x value\n",
    "\n",
    "                # initialize score, which will help determine which cluster to go to\n",
    "                temppp = 2*np.ones(self.Nk.shape[0], dtype=np.int64)\n",
    "                score = np.hstack([np.log(temppp+1e-16), np.log(self.alpha+1e-16)])\n",
    "\n",
    "                \n",
    "                for i in np.arange(self.Nk.shape[0]): # go to each cluster (prob better to exclude empty ones, etc.)\n",
    "                    \n",
    "                    temp_S = self.X[self.z == i].reshape(-1,1) # the x values in that cluster\n",
    "                    temp_temp = np.sum(kernel(temp_x, temp_S)) # closeness of the point to that cluster\n",
    "\n",
    "                    # replace the initial value\n",
    "                    if global_turbine == False:\n",
    "                        score[i] = np.log(temp_temp+1e-16) \n",
    "                    else:\n",
    "                        score[i] = temp_temp\n",
    "                 \n",
    "                if global_turbine == False:\n",
    "                    # normalize\n",
    "                    temp_arr = np.array(score)\n",
    "                    score = (temp_arr - temp_arr.min()) / (temp_arr.max() - temp_arr.min()) # replace previous score\n",
    "                else:\n",
    "                    # a better way (assuming two experts)\n",
    "                    temp_arr = np.array(score)[:-1]\n",
    "                    temp_arr_max = temp_arr[np.argmax(temp_arr)]\n",
    "                    temp_arr_min = temp_arr[np.argmin(temp_arr)]\n",
    "                    temp_arr_diff = (temp_arr_max - temp_arr_min) / temp_arr_max # the smaller the percentage, the less of a difference\n",
    "                    score[np.argmax(temp_arr)] = temp_arr_diff\n",
    "                    score[np.argmin(temp_arr)] = 0.0\n",
    "\n",
    "                for i in np.arange(self.Nk.shape[0]): \n",
    "                    score[i] = 5*score[i] # now ranges from 0-5\n",
    "                    \n",
    "                # -------------------------------- now consider the goodness of fit --------------------------------------\n",
    "                fit_scores = []\n",
    "                for k in active_comp_ids[:-1]: # only go through the clusters that satisfy the conditions\n",
    "                    \n",
    "                    temp_true = self.Y[n, ~np.isnan(self.Y[n])].reshape(-1, 1) # y value of the current point\n",
    "                    temp_pred = self.obsmodel[k].to_predict(self.X[n, ~np.isnan(self.X[n])].reshape(-1, 1)) # predict for the current point\n",
    "                    temp_score = 1/((((temp_true - temp_pred)** 2).mean())+1e-16) # a measure of the goodness of fit\n",
    "\n",
    "                    if global_turbine == False:\n",
    "                        fit_scores.append(np.log(float(temp_score)+1e-16)) # no longer a prob value\n",
    "                    else:\n",
    "                        fit_scores.append(float(temp_score))\n",
    "\n",
    "\n",
    "                new_comp = active_comp_ids[-1]\n",
    "                fit_scores.append(float(np.log(0*new_comp_model.current_score()+1e-16))) # supressed the new expert\n",
    "\n",
    "                if global_turbine == False:\n",
    "                    # noramlize\n",
    "                    temp_arr = np.array(fit_scores)\n",
    "                    fit_scores_updated = ((temp_arr - temp_arr.min()) / (temp_arr.max() - temp_arr.min())).tolist()\n",
    "\n",
    "                else:\n",
    "                    # a better way (assuming two experts)\n",
    "                    fit_scores_updated = fit_scores\n",
    "                    temp_arr = np.array(fit_scores_updated)[:-1]\n",
    "                    temp_arr_max = temp_arr[np.argmax(temp_arr)]\n",
    "                    temp_arr_min = temp_arr[np.argmin(temp_arr)]\n",
    "                    temp_arr_diff = (temp_arr_max - temp_arr_min) / temp_arr_max # the smaller the percentage, the less of a difference\n",
    "                    fit_scores_updated[np.argmax(temp_arr)] = temp_arr_diff\n",
    "                    fit_scores_updated[np.argmin(temp_arr)] = 0.0\n",
    "\n",
    "\n",
    "                temp_count = 0\n",
    "                for k in active_comp_ids: \n",
    "                    score[k] += 5*fit_scores_updated[temp_count]\n",
    "                    temp_count += 1\n",
    "                \n",
    "                final_score = score[active_comp_ids]\n",
    "\n",
    "                # Directly computing sum of exp(a) can lead to numerical issues such as overflow or underflow\n",
    "                # especially when the elements of a are very large or very small. \n",
    "                log_normalizing_constant = lse(final_score) # log of sum of exp(a)\n",
    "\n",
    "                self.p[n] = np.exp(final_score - log_normalizing_constant) # exp(a) / sum of exp(a)\n",
    "\n",
    "                # make the highest value 1 and everybody else 0\n",
    "                temp_probs = np.zeros_like(self.p[n])\n",
    "                temp_index = np.argmax(self.p[n])\n",
    "                temp_probs[temp_index] = 1\n",
    "                    \n",
    "                \"\"\" ---------------------------------- end ------------------------------- \"\"\"\n",
    "\n",
    "                \"\"\" ---------------------------- updates ----------------------------\n",
    "                \"\"\"\n",
    "                \n",
    "                self.z[n] = npr.choice(active_comp_ids, p=temp_probs) # assign the current data point\n",
    "\n",
    "                if self.z[n] > self.Nk.shape[0]-1: # if going to a new expert\n",
    "                    self.Nk = np.hstack([self.Nk, 1])\n",
    "                else: # if going to an existing expert\n",
    "                    self.Nk[self.z[n]] += 1\n",
    "\n",
    "                if self.z[n] == new_comp: # if going to a new expert\n",
    "                    self.obsmodel[self.z[n]] = new_comp_model # add the new model\n",
    "                else: # if going to an existing model\n",
    "                    self.obsmodel[self.z[n]] = monotonic_poly(\n",
    "                                     np.vstack([self.obsmodel[self.z[n]].X, self.X[n, ~np.isnan(self.X[n])].reshape(-1, 1)]), \n",
    "                                     np.vstack([self.obsmodel[self.z[n]].Y, self.Y[n, ~np.isnan(self.Y[n])].reshape(-1, 1)]))\n",
    "                    self.obsmodel[self.z[n]].to_fit(indiv_mono_str=self.indiv_mono_str)\n",
    "                    self.obsmodel = update_variance(self.z, self.obsmodel)\n",
    "\n",
    "            \"\"\" ------ done looping data ----- \"\"\"\n",
    "                        \n",
    "            # print information\n",
    "            temp_active_comps = self.Nk[self.Nk > 0]\n",
    "            #print('Iter {}: {}'.format(l, temp_active_comps))\n",
    "\n",
    "        \"\"\" ------ done looping iterations ---- \"\"\"\n",
    "        \n",
    "#When you load a pickled instance, Python needs to be able to locate the class definition using the module \n",
    "#and class name that were stored when the objectwas pickled.\n",
    "MOGP = MOBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c49f79c-6ddd-4775-af61-1227cc7bb6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data():\n",
    "\n",
    "    choice = global_choice\n",
    "    np.random.seed(42)\n",
    "\n",
    "    if choice == 112.5: # wavy\n",
    "\n",
    "        x1 = np.linspace(0.1,0.45001,49)\n",
    "        x1 = np.append(x1, 0.48)\n",
    "        \n",
    "        term = 3*np.pi*1.5*x1\n",
    "        g_x = 1.05*(np.sin(term) + term)/(3*np.pi) + 1.0\n",
    "        y1 = g_x + np.random.normal(0, 0.01, x1.size)\n",
    "\n",
    "        #print(x1.shape)\n",
    "\n",
    "        x2 = np.linspace(0.55727919,0.9,50)\n",
    "        \n",
    "        x_adjust = x2 - x2[0] \n",
    "        y_adjust = 1.780394386232395 \n",
    "        y2 = y_adjust + 2.6e-6*x_adjust + np.random.normal(0, 0.01, x2.size)\n",
    "\n",
    "        #print(x2.shape)\n",
    "\n",
    "        #print('the intersection of the two model is (0.470251, 1.780394)')\n",
    "        #print(f'the middle between the two data clusters is {x1[-1]/2 + x2[0]/2}')\n",
    "\n",
    "        x_combined = np.concatenate([x1, x2]).reshape(-1,1)\n",
    "        y_combined = np.concatenate([y1, y2])\n",
    "    \n",
    "        X = x_combined\n",
    "        Y = y_combined\n",
    "\n",
    "\n",
    "    elif choice == 200: # material science\n",
    "        \n",
    "        with open('data/material_sci_data.pkl', 'rb') as f:\n",
    "            mega_summ_stat = pickle.load(f)\n",
    "\n",
    "        summ_stat = mega_summ_stat[0]\n",
    "\n",
    "        CF_xs = summ_stat[0]\n",
    "        CF_ns = summ_stat[1]\n",
    "        CF_means = summ_stat[2]\n",
    "        CF_stds = summ_stat[3]\n",
    "    \n",
    "        DF_xs = summ_stat[4]\n",
    "        DF_ns = summ_stat[5]\n",
    "        DF_means = summ_stat[6]\n",
    "        DF_stds = summ_stat[7]\n",
    "\n",
    "        x1 = CF_xs\n",
    "        x1 = x1[~np.isnan(x1)]\n",
    "        x1 = (x1 - 0) / (70 - 0)\n",
    "        #print(len(x1))\n",
    "        \n",
    "        SE = np.zeros(len(CF_stds))\n",
    "        for i in range(0, len(CF_stds)):\n",
    "            if CF_ns[i] == 0:\n",
    "                SE[i] = np.nan\n",
    "            else:\n",
    "                SE[i] = CF_stds[i] / np.sqrt(CF_ns[i]) \n",
    "\n",
    "        y1 = np.random.normal(CF_means, SE)\n",
    "        y1 = y1[~np.isnan(y1)]\n",
    "\n",
    "        x2 = DF_xs\n",
    "        x2 = x2[~np.isnan(x2)]\n",
    "        x2 = (x2 - 0) / (70 - 0)\n",
    "        #print(len(x2))\n",
    "\n",
    "        SE = np.zeros(len(DF_stds))\n",
    "        for i in range(0, len(DF_stds)):\n",
    "            if DF_ns[i] == 0:\n",
    "                SE[i] = np.nan\n",
    "            else:\n",
    "                SE[i] = DF_stds[i] / np.sqrt(DF_ns[i])\n",
    "\n",
    "        y2 = np.random.normal(DF_means, SE)\n",
    "        y2 = y2[~np.isnan(y2)]\n",
    "        \n",
    "        x_combined = np.concatenate([x1, x2]).reshape(-1,1)\n",
    "        y_combined = np.concatenate([y1, y2])\n",
    "    \n",
    "        X = x_combined\n",
    "        X = add_turbulence2(X.reshape(-1).copy()).reshape(-1,1)\n",
    "        Y = y_combined\n",
    "\n",
    "\n",
    "    elif choice == 304.555: # turbine\n",
    "        \n",
    "        df = pd.read_pickle('data/turbine_train_data.pkl')\n",
    "        \n",
    "        x = np.array(df.iloc[:,0])\n",
    "        x = (x - 3.5) / (19.9 - 3.5) # normalize\n",
    "        y = np.array(df.iloc[:,1])\n",
    "        \n",
    "        X = x.reshape(-1,1)\n",
    "        X = add_turbulence(X.reshape(-1).copy()).reshape(-1,1)\n",
    "        Y = y\n",
    "\n",
    "        \n",
    "    #print(X.shape, Y.shape)\n",
    "\n",
    "    # Plot the data for visualization\n",
    "    # plt.figure(figsize=(10, 6))\n",
    "    # plt.scatter(X, Y, s=10)\n",
    "    # plt.title('Generated Data')\n",
    "    # plt.xlabel('X')\n",
    "    # plt.ylabel('Y')\n",
    "    # plt.grid(True)\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37264da0-0de6-46a0-ac82-f6819cd2db26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test(X, Y, c_test):\n",
    "\n",
    "    \"\"\"\n",
    "    to generate test data\n",
    "    note: \n",
    "     - do not use random_state of 42\n",
    "     - should not alter X or Y\n",
    "\n",
    "    \"\"\"\n",
    "    random_state = 1\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "\n",
    "\n",
    "    if c_test == 1112: # test data for wavy\n",
    "\n",
    "        X_test = []\n",
    "        Y_test = []\n",
    "        \n",
    "        for test_i in range(0,30):\n",
    "            \n",
    "            true_threshold = 0.470251\n",
    "            x1 = np.zeros(25)\n",
    "            even_x1 = np.sort(np.random.uniform(0.1,true_threshold-0.0001,x1.size)) \n",
    "\n",
    "            np.random.normal(0, 0.01, x1.size) # for reproducibility\n",
    "            \n",
    "            term = 3*np.pi*1.5*even_x1 \n",
    "            g_x = 1.05*(np.sin(term) + term)/(3*np.pi) + 1.0\n",
    "            y1 = g_x + np.random.normal(0, 0.01, even_x1.size)\n",
    "\n",
    "            x2 = np.zeros(25)\n",
    "            even_x2 = np.sort(np.random.uniform(true_threshold+0.0001,0.9,x2.size))\n",
    "            \n",
    "            x_adjust = even_x2 - 0.55727919\n",
    "            y_adjust = 1.780394386232395 \n",
    "            y2 = y_adjust + 2.6e-6*x_adjust + np.random.normal(0, 0.01, even_x2.size)\n",
    "            \n",
    "            x_combined = np.concatenate([even_x1, even_x2]).reshape(-1,1)\n",
    "            y_combined = np.concatenate([y1, y2])\n",
    "        \n",
    "            X_test_current = x_combined\n",
    "            Y_test_current = y_combined\n",
    "\n",
    "            X_test.append(X_test_current)\n",
    "            Y_test.append(Y_test_current)\n",
    "\n",
    "    elif c_test == 1200: # test data for material science\n",
    "\n",
    "        X_test = []\n",
    "        Y_test = []\n",
    "\n",
    "        for test_i in range(0,30):\n",
    "        \n",
    "            with open('data/material_sci_data.pkl', 'rb') as f:\n",
    "                mega_summ_stat = pickle.load(f)\n",
    "    \n",
    "            summ_stat = mega_summ_stat[0]\n",
    "    \n",
    "            CF_xs = summ_stat[0]\n",
    "            CF_ns = summ_stat[1]\n",
    "            CF_means = summ_stat[2]\n",
    "            CF_stds = summ_stat[3]\n",
    "        \n",
    "            DF_xs = summ_stat[4]\n",
    "            DF_ns = summ_stat[5]\n",
    "            DF_means = summ_stat[6]\n",
    "            DF_stds = summ_stat[7]\n",
    "    \n",
    "            x1 = CF_xs\n",
    "            x1 = x1[~np.isnan(x1)]\n",
    "            x1 = (x1 - 0) / (70 - 0) + np.random.normal(0,0.0001,x1.size)\n",
    "            x1[x1 < 0] = 0\n",
    "            x1[x1 > 1] = 1\n",
    "            #print(len(x1))\n",
    "            \n",
    "            SE = np.zeros(len(CF_stds))\n",
    "            for i in range(0, len(CF_stds)):\n",
    "                if CF_ns[i] == 0:\n",
    "                    SE[i] = np.nan\n",
    "                else:\n",
    "                    SE[i] = CF_stds[i] / np.sqrt(CF_ns[i]) \n",
    "    \n",
    "            y1 = np.random.normal(CF_means, SE)\n",
    "            y1 = y1[~np.isnan(y1)]\n",
    "    \n",
    "            x2 = DF_xs\n",
    "            x2 = x2[~np.isnan(x2)]\n",
    "            x2 = (x2 - 0) / (70 - 0) + np.random.normal(0,0.0001,x2.size)\n",
    "            x2[x2 < 0] = 0\n",
    "            x2[x2 > 1] = 1\n",
    "            #print(len(x2))\n",
    "    \n",
    "            SE = np.zeros(len(DF_stds))\n",
    "            for i in range(0, len(DF_stds)):\n",
    "                if DF_ns[i] == 0:\n",
    "                    SE[i] = np.nan\n",
    "                else:\n",
    "                    SE[i] = DF_stds[i] / np.sqrt(DF_ns[i])\n",
    "    \n",
    "            y2 = np.random.normal(DF_means, SE)\n",
    "            y2 = y2[~np.isnan(y2)]\n",
    "            \n",
    "            x_combined = np.concatenate([x1, x2]).reshape(-1,1)\n",
    "            y_combined = np.concatenate([y1, y2])\n",
    "        \n",
    "            X_test_current = x_combined\n",
    "            X_test_current = add_turbulence2(X_test_current.reshape(-1).copy()).reshape(-1,1)\n",
    "            Y_test_current = y_combined\n",
    "\n",
    "            indices = np.random.choice(len(X_test_current), size=10, replace=False)\n",
    "\n",
    "            X_test.append(X_test_current[indices])\n",
    "            Y_test.append(Y_test_current[indices])\n",
    "\n",
    "\n",
    "    elif c_test == 1304: # filtered test data for turbine\n",
    "\n",
    "        with open(\"data/turbine_test_data_filtered.pkl\", \"rb\") as f:\n",
    "            # Load the pickled list from the file\n",
    "            df_list = pickle.load(f)\n",
    "\n",
    "        X_test = []\n",
    "        Y_test = []\n",
    "\n",
    "        for df in df_list:\n",
    "\n",
    "            df = df[df.iloc[:, 0] <= 19.9]\n",
    "            \n",
    "            x = np.array(df.iloc[:,0])\n",
    "            x = (x - 3.5) / (19.9 - 3.5) # normalize in the same way\n",
    "            y = np.array(df.iloc[:,1])\n",
    "            \n",
    "            X_test_current = x.reshape(-1,1)\n",
    "            X_test_current = add_turbulence(X_test_current.reshape(-1).copy()).reshape(-1,1)\n",
    "            Y_test_current = y\n",
    "\n",
    "            X_test.append(X_test_current)\n",
    "            Y_test.append(Y_test_current)\n",
    "\n",
    "    elif c_test == 1304.1: # unfiltered test data for turbine\n",
    "\n",
    "        with open(\"data/turbine_test_data_unfiltered.pkl\", \"rb\") as f:\n",
    "            # Load the pickled list from the file\n",
    "            df_list = pickle.load(f)\n",
    "\n",
    "        X_test = []\n",
    "        Y_test = []\n",
    "\n",
    "        for df in df_list:\n",
    "\n",
    "            df = df[df.iloc[:, 0] <= 19.9]\n",
    "            \n",
    "            x = np.array(df.iloc[:,0])\n",
    "            x = (x - 3.5) / (19.9 - 3.5) # normalize in the same way\n",
    "            y = np.array(df.iloc[:,1])\n",
    "            \n",
    "            X_test_current = x.reshape(-1,1)\n",
    "            X_test_current = add_turbulence(X_test_current.reshape(-1).copy()).reshape(-1,1)\n",
    "            Y_test_current = y\n",
    "\n",
    "            X_test.append(X_test_current)\n",
    "            Y_test.append(Y_test_current)\n",
    "\n",
    "\n",
    "    X_train = X\n",
    "    Y_train = Y\n",
    "\n",
    "    # Plot the data for visualization\n",
    "    # plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # if isinstance(X_test, list) != True:\n",
    "    #     plt.scatter(X_test, Y_test, s=10)\n",
    "    # else:\n",
    "    #     plt.scatter(X_test[0], Y_test[0], s=10)\n",
    "        \n",
    "    # plt.title('Generated Data')\n",
    "    # plt.xlabel('X')\n",
    "    # plt.ylabel('Y')\n",
    "    # plt.grid(True)\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c87524-6e89-4777-b9ce-b901c9d1337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_result2(mixing, mixed_X, mixed_Y, clusters_X=None, clusters_Y=None, mixed_X_test=None, mixed_Y_test=None):\n",
    "\n",
    "    \"\"\"\n",
    "    mixed_X_test, mixed_Y_test: optional testing data to predict for, otherwise we are using testing values across the domain\n",
    "    \n",
    "    \"\"\"\n",
    "    X = np.linspace(0,1,1000).reshape(-1,1) # the x values across the domain to predict for\n",
    "    Y_true = None\n",
    "\n",
    "    if mixed_Y_test is not None:\n",
    "        temp_index = np.argsort(mixed_X_test.reshape(-1))\n",
    "        X = mixed_X_test[temp_index].reshape(-1,1) # sorted testing x values\n",
    "        Y_true = mixed_Y_test[temp_index] # sorted testing y values\n",
    "    \n",
    "    z = mixing.z\n",
    "    unique_z = unique_preserve_order(z)\n",
    "    \n",
    "    # clustering metrics\n",
    "    # data_2D = np.concatenate((mixed_X, mixed_Y.reshape(-1,1)), axis=1)\n",
    "    # dbi = davies_bouldin_score(data_2D, mixing.z)\n",
    "    # print(f'Davies-Bouldin Index: {dbi:.4f}')\n",
    "    # true_labels = np.concatenate([np.ones(42, dtype=int),np.zeros(44, dtype=int)])\n",
    "    # ari_metric = adjusted_rand_score(true_labels, mixing.z)\n",
    "    # print(f'Adjusted Rand Index (ARI): {ari_metric:.4f}')\n",
    "    # accuracy = accuracy_score(true_labels, mixing.z)\n",
    "    # print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    means_matrix = [] # pred means from all models\n",
    "    vars_matrix = [] # pred variances from all models\n",
    "    for i in np.arange(unique_z.shape[0]):\n",
    "        \n",
    "        mean = mixing.obsmodel[unique_z[i]].to_predict(X)\n",
    "        means_matrix.append(mean.reshape(-1))\n",
    "        \n",
    "        var = mixing.obsmodel[unique_z[i]].to_predict_var(X)\n",
    "        var = np.diag(var)\n",
    "        vars_matrix.append(var.reshape(-1))\n",
    "\n",
    "\n",
    "    def kernel(x, y, gamma=global_gamma):\n",
    "        return np.exp(-gamma * (x - y)**2)\n",
    "    \n",
    "    weights_matrix = [] # the closeness of all test points to all clusters, # test points by # clusters\n",
    "    \n",
    "    for x in X:\n",
    "        raw_weights = [] # the closeness of current test point to all clusters\n",
    "        for i in np.arange(unique_z.shape[0]):\n",
    "\n",
    "            current_set = mixed_X[mixing.z == unique_z[i],:].reshape(-1) # the x values from this cluster\n",
    "            temp_weight1 = np.sum(kernel(x, current_set)) # the closeness of current test point to the cluster\n",
    "            raw_weights.append(temp_weight1)\n",
    "\n",
    "        normalizing_constant = np.sum(raw_weights) \n",
    "        weights = raw_weights / (normalizing_constant)\n",
    "        \n",
    "        weights_matrix.append(weights)\n",
    "\n",
    "    gate_to_plot = [float(element[0]) for element in weights_matrix]\n",
    "    \n",
    "    weighted_means = [] # mixture prediction\n",
    "    weighted_vars = []\n",
    "    for i in range(len(X)): \n",
    "        \n",
    "        weights = weights_matrix[i] # the closeness of current test point to all clusters\n",
    "        weights_vars = [item**2 for item in weights] \n",
    "\n",
    "        model_means = [model[i] for model in means_matrix] # prediction for this test point from each model\n",
    "        model_vars = [model[i] for model in vars_matrix] \n",
    "\n",
    "        weighted_means.append(float(sum([a * b for a, b in zip(model_means, weights)]))) \n",
    "        weighted_vars.append(float(sum([a * b for a, b in zip(model_vars, weights_vars)])))  \n",
    "    \n",
    "    actual_y = weighted_means\n",
    "    \n",
    "    def quantifiable_results(y_true, y_pred, x):\n",
    "\n",
    "        x = x.reshape(-1) # test x \n",
    "        y_pred = np.array(y_pred) # predicted y value for the test x\n",
    "\n",
    "        if y_true is not None: # the true y value for the test x\n",
    "            y_true = np.array(y_true)\n",
    "            mse = mean_squared_error(y_true, y_pred)\n",
    "            mae = None\n",
    "            r2 = None\n",
    "        else:\n",
    "            mse = None\n",
    "            mae = None\n",
    "            r2 = None\n",
    "\n",
    "        y_pred_rounded = np.round(y_pred, decimals=5)\n",
    "        correlation, _ = spearmanr(x, y_pred_rounded)\n",
    "        monotonicity_score = correlation\n",
    "\n",
    "        if mixed_X_test is None:\n",
    "            print(f\"Monotonicity Score: {monotonicity_score}\")\n",
    "\n",
    "        return mse, mae, r2\n",
    "\n",
    "    mse, mae, r2 = quantifiable_results(Y_true, actual_y, X)\n",
    "    \n",
    "    weighted_stds = [np.sqrt(item) for item in weighted_vars] # mixture prediction standard deviation\n",
    "\n",
    "    lbs = [weighted_means[i] - 1.96 * weighted_stds[i] for i in range(len(X))] # mixture prediction means as the center\n",
    "    ubs = [weighted_means[i] + 1.96 * weighted_stds[i] for i in range(len(X))]\n",
    "\n",
    "\n",
    "    return [X, actual_y, Y_true, mse, mae, r2, gate_to_plot, lbs, ubs]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ce4697-75bd-42a4-8f4d-a1ce66d2b9ee",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_result(z, mixed_X, mixed_Y, clusters_X=None, clusters_Y=None, option=\"both + mixture_pred\", plot_mixture_pred=None, case_study=\"wavy\", save_file=False):\n",
    "\n",
    "    \"\"\"\n",
    "    mixed_X and mixed_Y are the training data \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    unique_z = unique_preserve_order(z) \n",
    "    \n",
    "    means = [] # predictions for all sorted training x values, from each model\n",
    "    vars = []\n",
    "    sub_means = [] # predictions for sorted training x values from each cluster, from corresponding model\n",
    "    sub_vars = []\n",
    "    \n",
    "    sub_Xs = [] # list of training x values from each cluster (sorted)\n",
    "    sub_Xs_original = [] # list of training x values from each cluster (without sorting)\n",
    "    sub_Ys_original = []\n",
    "    \n",
    "    entire_X = np.sort(mixed_X.reshape(-1,1), axis=0) # sorted training x values (from all clusters)\n",
    "    entire_X_original = mixed_X.reshape(-1,1) # without sorting\n",
    "    entire_Y_original = mixed_Y.reshape(-1,1) \n",
    "    \n",
    "    for i in np.arange(unique_z.shape[0]):\n",
    "        \n",
    "        index = np.where(z == unique_z[i])[0]\n",
    "\n",
    "        # all the data points of a particular cluster\n",
    "        X = np.sort(mixed_X[index].reshape(-1,1), axis=0) # sorted training x values from this cluster\n",
    "        X_original = mixed_X[index].reshape(-1,1) # without sorting\n",
    "        Y_original = mixed_Y[index].reshape(-1,1) # without soting\n",
    "        sub_Xs_original.append(X_original)\n",
    "        sub_Ys_original.append(Y_original)\n",
    "        \n",
    "        if X[~np.isnan(X)].shape[0] > 0:\n",
    "            \n",
    "            mean = mixing.obsmodel[unique_z[i]].to_predict(entire_X) \n",
    "            means.append(mean)\n",
    "            \n",
    "            sub_Xs.append(X)\n",
    "            sub_mean = mixing.obsmodel[unique_z[i]].to_predict(X)\n",
    "            sub_means.append(sub_mean)\n",
    "\n",
    "            sub_var = mixing.obsmodel[unique_z[i]].to_predict_var(X)\n",
    "            sub_vars.append(np.diag(sub_var))\n",
    "        \n",
    "    if plot_mixture_pred is not None:\n",
    "\n",
    "        plt.close()\n",
    "        colors = ['#ADD8E6','#FFC87C']\n",
    "        shapes = ['o', 's']\n",
    "        linestyles = ['--','-.']\n",
    "        \n",
    "        plt.style.use('default')\n",
    "        plt.figure(figsize=(10, 6), dpi=300)\n",
    "        \n",
    "        plt.xlabel('X', fontsize=22, labelpad=10)  \n",
    "        plt.ylabel('Y', fontsize=22, labelpad=10) \n",
    "        \n",
    "        plt.xticks(fontsize=20) \n",
    "        plt.yticks(fontsize=20) \n",
    "\n",
    "        # extract from the input\n",
    "        mixture_X = plot_mixture_pred[0]\n",
    "        mixture_Y = plot_mixture_pred[1]\n",
    "        mixture_Y_true = plot_mixture_pred[2]\n",
    "        gate_to_plot = plot_mixture_pred[6]\n",
    "        lbs = plot_mixture_pred[7]\n",
    "        ubs = plot_mixture_pred[8]\n",
    "\n",
    "        if case_study == \"turbine\":\n",
    "            mixture_X = mixture_X*(19.9-3.5)+3.5\n",
    "            sub_Xs_original = [item*(19.9-3.5)+3.5 for item in sub_Xs_original]\n",
    "\n",
    "        # Plot each data subset with distinct colors and markers\n",
    "        \n",
    "        for i in np.arange(unique_z.shape[0]):\n",
    "            i_replace = i\n",
    "            if case_study == \"material_sci\":\n",
    "                i_replace = i-1\n",
    "            plt.scatter(\n",
    "                sub_Xs_original[i_replace], sub_Ys_original[i_replace], # list of training points from each cluster (without sorting)\n",
    "                label=f'Assigned data for expert {i+1}', \n",
    "                color=colors[i],  \n",
    "                marker=shapes[i],\n",
    "                alpha=0.85,  # Slight transparency for clarity\n",
    "                s=60,  # Larger markers for better visibility\n",
    "            )\n",
    "        \n",
    "        # Plot the mean prediction line\n",
    "        plt.plot(\n",
    "            mixture_X, mixture_Y,\n",
    "            label='Mixture Prediction', color='black',\n",
    "            linewidth=2.5, linestyle='-', alpha=0.9\n",
    "        )\n",
    "\n",
    "        # Plotting the prediction interval\n",
    "        plt.fill_between(mixture_X[:, 0], \n",
    "                         lbs, \n",
    "                         ubs, \n",
    "                         alpha=0.5,  \n",
    "                         color='lightgrey', \n",
    "                         label='95% Mixture Prediction Interval') \n",
    "        \n",
    "        # Optionally, add dashed lines to highlight the bounds of the interval\n",
    "        # *(19.9-3.5)+3.5\n",
    "        plt.plot(mixture_X[:, 0], lbs, color='gray', linestyle='--', linewidth=1.5)  # Lower bound\n",
    "        plt.plot(mixture_X[:, 0], ubs, color='gray', linestyle='--', linewidth=1.5)  # Upper bound\n",
    "        \n",
    "\n",
    "        if case_study == \"material_sci\":\n",
    "            leg_loc = 'upper left'\n",
    "        else:\n",
    "            leg_loc = 'lower right'\n",
    "            \n",
    "        plt.legend(\n",
    "            loc=leg_loc, borderaxespad=1, # the padding between the legends border and the axes\n",
    "            prop={'size': 19},  \n",
    "            markerscale=1.5, # scales the marker size in the legend relative to their original size in the plot\n",
    "            frameon=True, \n",
    "            framealpha=0.8,  \n",
    "            facecolor='white',  \n",
    "            edgecolor='grey'   \n",
    "        )\n",
    "        \n",
    "        plt.tight_layout() # for minimal white space\n",
    "\n",
    "        if case_study == \"wavy\":\n",
    "            plt.ylim(1.1,2.01)\n",
    "            plt.xlim(0.09,0.91)\n",
    "\n",
    "        if case_study == \"material_sci\":\n",
    "            plt.ylim(-15,55)\n",
    "            plt.xlim(0,0.95)\n",
    "\n",
    "        if case_study == \"turbine\":\n",
    "            plt.ylim(-0.3,1.3)\n",
    "            plt.xlim(3.5,19.3)\n",
    "\n",
    "        if save_file == True:\n",
    "            plt.savefig('publication_figures/'+case_study+'_baseline_'+indiv_mono_str+'.png', dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "    return\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae8c19b-daa3-4270-9579-babb01d91a5b",
   "metadata": {},
   "source": [
    "## Function Calls and Results\n",
    "*Choose the dataset and model to see the corresponding figure, monotonicity score, average MSE, and standard error of MSE*\n",
    "- *Dataset choices: ```\"wavy\"``` for synthetic data, ```\"material_sci\"```, and ```\"turbine\"```*\n",
    "- *Model choices: ```\"no_mono\"``` for Baseline MoE and ```\"yes_mono\"``` for MMIE*\n",
    "- *Results will appear in the output, set ```save_file``` to be ```True``` if wishing to save the figure to the ```publication_figures/``` folder*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7db26da-1090-47cc-a6bb-38ea806174a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure\n",
    "indiv_mono_str = \"yes_mono\"\n",
    "case_study = \"material_sci\"\n",
    "save_file = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b584ff5-182d-4010-83ec-fb81cca3868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to generate results\n",
    "if case_study == \"material_sci\":\n",
    "    global_gamma = 2000\n",
    "    global_choice = 200\n",
    "    global_c_test = 1200\n",
    "    global_first_cluster_size = 44\n",
    "    global_degree = 20\n",
    "    global_alpha = 2\n",
    "    global_num_iter = 5\n",
    "    global_ub = 0.6\n",
    "    global_lb = 0.4\n",
    "    global_turbine = False\n",
    "elif case_study == \"wavy\":\n",
    "    global_gamma = 200#5000\n",
    "    global_choice = 112.5\n",
    "    global_c_test = 1112\n",
    "    global_first_cluster_size = 50\n",
    "    global_degree = 40\n",
    "    global_alpha = 0.5\n",
    "    global_num_iter = 5\n",
    "    global_ub = 0.7\n",
    "    global_lb = 0.4\n",
    "    global_turbine = False\n",
    "elif case_study == \"turbine\":\n",
    "    global_gamma = 5000\n",
    "    global_choice = 304.555\n",
    "    global_c_test_filtered = 1304\n",
    "    global_c_test_unfiltered = 1304.1\n",
    "    global_first_cluster_size = 425\n",
    "    global_degree = 15\n",
    "    global_alpha = 2\n",
    "    global_num_iter = 20\n",
    "    global_ub = 0.7\n",
    "    global_lb = 0.3\n",
    "    global_turbine = True\n",
    "\n",
    "\n",
    "# generate training data\n",
    "mixed_X, mixed_Y = generate_data()\n",
    "# generate test data\n",
    "if case_study == \"turbine\":\n",
    "    mixed_X_train, mixed_X_test_filtered, mixed_Y_train, mixed_Y_test_filtered = generate_test(mixed_X, mixed_Y, global_c_test_filtered)\n",
    "    _, mixed_X_test_unfiltered, __, mixed_Y_test_unfiltered = generate_test(mixed_X, mixed_Y, global_c_test_unfiltered)\n",
    "else:\n",
    "    mixed_X_train, mixed_X_test, mixed_Y_train, mixed_Y_test = generate_test(mixed_X, mixed_Y, global_c_test)\n",
    "# initialize the mixture model\n",
    "mixing = MOBP(X=mixed_X_train, Y=mixed_Y_train, alpha=0, num_init_clusters=2, num_iter=global_num_iter, indiv_mono_str=indiv_mono_str) \n",
    "# fit the mixture model\n",
    "if case_study != \"turbine\":\n",
    "    temp_start = time.time()\n",
    "    mixing.sample()\n",
    "    temp_end = time.time()\n",
    "    print(f'time took: {temp_end - temp_start} seconds')\n",
    "else:\n",
    "    with open(\"baseline_fitted_models/\"+indiv_mono_str+\"_turbine.pkl\", \"rb\") as f:\n",
    "        mixing = pickle.load(f)\n",
    "\n",
    "# generate what is needed for figure\n",
    "plot_items = visualize_result2(mixing, mixed_X_train, mixed_Y_train, mixed_X_test=None, mixed_Y_test=None)\n",
    "\n",
    "# generate the metrics\n",
    "if case_study == \"turbine\":\n",
    "    total_num_seeds = len(mixed_X_test_filtered)\n",
    "    \n",
    "    mse_list = []\n",
    "    for test_i in range(0, total_num_seeds):\n",
    "        plot_items_ = visualize_result2(mixing, mixed_X_train, mixed_Y_train, mixed_X_test=mixed_X_test_filtered[test_i], mixed_Y_test=mixed_Y_test_filtered[test_i])\n",
    "        mse_list.append(plot_items_[3])\n",
    "\n",
    "    print('For filtered test set:')\n",
    "    print(f'average mse over {total_num_seeds} seeds is {sum(mse_list)/total_num_seeds}')\n",
    "    print(f'SE of mse is {np.std(mse_list)/np.sqrt(total_num_seeds)}')\n",
    "\n",
    "    # repeat\n",
    "    mse_list = []\n",
    "    for test_i in range(0, total_num_seeds):\n",
    "        plot_items_ = visualize_result2(mixing, mixed_X_train, mixed_Y_train, mixed_X_test=mixed_X_test_unfiltered[test_i], mixed_Y_test=mixed_Y_test_unfiltered[test_i])\n",
    "        mse_list.append(plot_items_[3])\n",
    "\n",
    "    print('For unfiltered test set:')\n",
    "    print(f'average mse over {total_num_seeds} seeds is {sum(mse_list)/total_num_seeds}')\n",
    "    print(f'SE of mse is {np.std(mse_list)/np.sqrt(total_num_seeds)}')\n",
    "else:\n",
    "    total_num_seeds = len(mixed_X_test)\n",
    "    mse_list = []\n",
    "    \n",
    "    for test_i in range(0, total_num_seeds):\n",
    "        plot_items_ = visualize_result2(mixing, mixed_X_train, mixed_Y_train, mixed_X_test=mixed_X_test[test_i], mixed_Y_test=mixed_Y_test[test_i])\n",
    "        mse_list.append(plot_items_[3])\n",
    "        \n",
    "    print(f'average mse over {total_num_seeds} seeds is {sum(mse_list)/total_num_seeds}')\n",
    "    print(f'SE of mse is {np.std(mse_list)/np.sqrt(total_num_seeds)}')\n",
    "    \n",
    "# generate the figure\n",
    "visualize_result(mixing.z, mixed_X_train, mixed_Y_train, option='both + mixture_pred', plot_mixture_pred=plot_items, case_study=case_study, save_file=save_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5597c546-e8fc-4fc8-898f-99448ca21611",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5024366-7643-4c80-9c02-73b36423626b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
