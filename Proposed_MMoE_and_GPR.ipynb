{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b27d822b-66da-4258-bd5c-d8d3f9828c05",
   "metadata": {},
   "source": [
    "# Code File for Proposed MMoE and Standard GPR\n",
    "*Produce the figure, monotonicity score, average MSE, and standard error of MSE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbd2032-5003-4c2d-90db-424e248643cf",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "*Make sure you are in the correct coding environment and import dependencies*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db37fa3-fc5c-4f72-8642-921a50043ee7",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!which python3\n",
    "\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7965f121-1008-4171-aa07-4d21e9afd493",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "#(1)\n",
    "import GPy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from matplotlib.cm import get_cmap\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "\n",
    "#(2)\n",
    "import math\n",
    "import numbers\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import numpy.random as npr\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.special import logsumexp as lse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from GPy.mappings import Linear\n",
    "\n",
    "import numpy\n",
    "import scipy\n",
    "import time\n",
    "import copy\n",
    "import pickle\n",
    "import warnings\n",
    "import cvxpy as cp\n",
    "from scipy.stats import t\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import binom\n",
    "from scipy.special import comb\n",
    "from scipy.optimize import fsolve\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "#from math import comb, just use math.comb\n",
    "from scipy.interpolate import PchipInterpolator\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.metrics import davies_bouldin_score, adjusted_rand_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, WhiteKernel\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1dd121-bcb7-4876-ad7d-f8d2bd122ed8",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "*Run the following cells to define them*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6095ef57-62e5-4113-8e4f-686b8cec0dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_turbulence(arr, seed=1):\n",
    "    \"\"\"\n",
    "    Function to add turbulence to duplicate elements\n",
    "    \"\"\"\n",
    "    unique, counts = np.unique(arr, return_counts=True)\n",
    "    duplicates = unique[counts > 1]\n",
    "    \n",
    "    for duplicate in duplicates:\n",
    "        # Find all indices of the duplicate element\n",
    "        indices = np.where(arr == duplicate)[0]\n",
    "        # Add small random noise to all but the first occurrence\n",
    "        for idx in indices[1:]:\n",
    "            np.random.seed(seed) \n",
    "            arr[idx] += np.random.uniform(1e-10, 1e-9)  # small random noise\n",
    "    return arr\n",
    "\n",
    "def add_turbulence_new(arr, seed=1):\n",
    "    \"\"\"\n",
    "    Function to add turbulence to duplicate elements\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    unique, counts = np.unique(arr, return_counts=True)\n",
    "    duplicates = unique[counts > 1]\n",
    "    \n",
    "    for duplicate in duplicates:\n",
    "        # Find all indices of the duplicate element\n",
    "        indices = np.where(arr == duplicate)[0]\n",
    "        # Add small random noise to all but the first occurrence\n",
    "        for idx in indices[1:]: \n",
    "            arr[idx] += np.random.uniform(1e-10, 1e-9)  # small random noise\n",
    "            \n",
    "    return arr\n",
    "\n",
    "def test_add_turbulence():\n",
    "\n",
    "    # Original array\n",
    "    original_array = np.array([1.0, 2.0, 2.0, 3.0, 4.0, 4.0, 4.0, 5.0])\n",
    "    \n",
    "    # Apply turbulence\n",
    "    modified_array = add_turbulence(original_array.copy())\n",
    "    \n",
    "    print(\"Original array:\", original_array)\n",
    "    print(\"Modified array:\", modified_array)\n",
    "\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd90652-063a-4b04-97f8-587f286fda2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class monotonic_poly:\n",
    "\n",
    "    \"\"\"\n",
    "    the class for the BP model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, X, Y): \n",
    "\n",
    "        # X: (n, 1) \n",
    "        # Y: (n,)\n",
    "\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.deg = global_degree\n",
    "        self.u_opt = np.zeros(self.deg+1)\n",
    "        self.var = np.eye(self.deg+1) # coefficient variance\n",
    "        self.noise = 1\n",
    "\n",
    "    def bernvander(self, x, deg):\n",
    "        \"\"\"\n",
    "        the binomial pmf with \"deg\" trials and the probability given by \"x\"\n",
    "        the berstein basis B(deg,x)\n",
    "        returns a 2D array, a design matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        return binom.pmf(np.arange(1 + deg), deg, x.reshape(-1, 1))\n",
    "\n",
    "\n",
    "    def to_fit(self):\n",
    "\n",
    "        x = self.X.reshape(-1)\n",
    "        y = self.Y.reshape(-1)\n",
    "                               \n",
    "        deg = global_degree\n",
    "        alpha = global_alpha # penalty for smoothness\n",
    "        \n",
    "        u = cp.Variable(deg + 1) # a placeholder for the optimal coefficients\n",
    "        loss = cp.sum_squares(self.bernvander(x, deg) @ u - y) # the sum of residual squares\n",
    "        reg = alpha * cp.sum_squares(cp.diff(u, 2))       # penalty for 2nd order differences\n",
    "        constraints = [cp.diff(u) >= 0]                   # constraints - u_{i+1} - u_i >= 0\n",
    "        problem = cp.Problem(cp.Minimize(loss + reg), constraints)\n",
    "        \n",
    "        problem.solve()\n",
    "        u_opt = u.value\n",
    "\n",
    "        self.deg = deg\n",
    "        self.u_opt = u_opt\n",
    "    \n",
    "        return\n",
    "\n",
    "    def to_predict(self, x_test):\n",
    "\n",
    "        # x_test: (n, 1)\n",
    "        # y_pred: (n,)\n",
    "\n",
    "        x_test = x_test.reshape(-1)\n",
    "        y_pred = self.bernvander(x_test, self.deg) @ self.u_opt # using the current coefficients\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "    def to_predict_var(self, x_test):\n",
    "\n",
    "        x_test = x_test.reshape(-1)\n",
    "        J = self.bernvander(x_test, self.deg)\n",
    "        \n",
    "        y_var = J @ self.var @ J.T + self.noise # add a noise to go from confidence interval to prediction interval\n",
    "\n",
    "        # print(f'are they equal? {np.array_equal(J @ (self.var+self.noise) @ J.T, y_var)}')\n",
    "        # print(f'one mean and std are {np.mean(J @ (self.var+self.noise) @ J.T)} and {np.std(J @ (self.var+self.noise) @ J.T)}')\n",
    "        # print(f'another mean and std are {np.mean(y_var)} and {np.std(y_var)}')\n",
    "        \n",
    "        return y_var\n",
    "\n",
    "    def current_score(self):\n",
    "\n",
    "        \"\"\"\n",
    "        provide a measure of how good the current data fits the current model\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.X.reshape(-1)\n",
    "        temp_pred = self.to_predict(x)\n",
    "        temp_true = self.Y\n",
    "\n",
    "        MSE = ((temp_true - temp_pred)** 2).mean()\n",
    "        output = 1/(MSE+1e-16)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "def test_poly():\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    x_train = np.sort(np.random.rand(100)).reshape(-1, 1)\n",
    "    y_train = (2 * x_train).ravel() + 0.3 * np.random.randn(100)\n",
    "\n",
    "    x_test = np.linspace(0, 1, 1000).reshape(-1, 1)\n",
    "\n",
    "    model = monotonic_poly(x_train, y_train)\n",
    "    model.to_fit()\n",
    "\n",
    "    y_pred = model.to_predict(x_test)\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(x_train, y_train, label='Training data')\n",
    "    plt.plot(x_test, y_pred, color='red', label='monotonic poly prediction')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend()\n",
    "    plt.title('Monotonic Poly Model')\n",
    "    plt.show()\n",
    "\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3777fe3-3612-40f2-a439-93bf41e4065e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_preserve_order(arr):\n",
    "    \"\"\"\n",
    "    grab the unique elements from the array while preserving order\n",
    "    \"\"\"\n",
    "    unique_dict = dict.fromkeys(arr)\n",
    "    # creates a new dictionary where the keys are the elements from arr\n",
    "    # dictionary keys must be unique\n",
    "    # In Python 3.7 and later, dictionaries maintain the order of insertion\n",
    "    \n",
    "    return np.array(list(unique_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee07d53a-7ee0-4d18-a8c2-9a4c9cfceb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_for_g_and_h_5(a, a_prime, points, xs, ys, x0):\n",
    "\n",
    "    \"\"\"\n",
    "    Proposed convex optimization (assume two experts)\n",
    "    x0 is the coefficients for all experts, only length is needed\n",
    "    \"\"\"\n",
    "\n",
    "    def binomial_prob(n, i, x):\n",
    "        \"\"\"\n",
    "        bionomial PMF for only i and only 1 x\n",
    "        \"\"\"\n",
    "        return comb(n, i) * (x ** i) * ((1 - x) ** (n - i))\n",
    "\n",
    "    def bernvander(x, deg):\n",
    "        \"\"\"\n",
    "        the binomial pmf with \"deg\" trials and the probability given by \"x\"\n",
    "        the berstein basis B(deg,x)\n",
    "        returns a 2D array, a design matrix\n",
    "        \"\"\"\n",
    "        return binom.pmf(np.arange(1 + deg), deg, x.reshape(-1, 1))\n",
    "\n",
    "    def obtain_C(n, a, a_prime, points):\n",
    "\n",
    "        \"\"\"\n",
    "        n is the number of coefficients\n",
    "        a is the weight for expert g at each grid point\n",
    "        \"points\" refers to \"grid points\"\n",
    "        \"\"\"\n",
    "\n",
    "        C_g = np.zeros((len(a),n))\n",
    "        C_h = np.zeros((len(a),n))\n",
    "        \n",
    "        for j in range(len(a)): # loop through each grid point\n",
    "\n",
    "            b_n_1_head = np.zeros(n) \n",
    "            for i in range(n - 1):\n",
    "                b_n_1_head[i+1] = binomial_prob(n-2, i, points[j]) # zero is at the head\n",
    "\n",
    "            b_n_1_toe = np.zeros(n) \n",
    "            for i in range(n - 1):\n",
    "                b_n_1_toe[i] = binomial_prob(n-2, i, points[j]) # zero is at the toe\n",
    "\n",
    "            b_n = np.zeros(n)\n",
    "            for i in range(n):\n",
    "                b_n[i] = binomial_prob(n-1, i, points[j])\n",
    "\n",
    "            C_g[j,:] = a[j] * b_n_1_head - a[j] * b_n_1_toe + a_prime[j] * b_n\n",
    "            C_h[j,:] = (1-a[j]) * b_n_1_head - (1-a[j]) * b_n_1_toe - a_prime[j] * b_n\n",
    "\n",
    "        return C_g, C_h\n",
    "\n",
    "\n",
    "    def constraint_function(x, n, C_g, C_h):\n",
    "\n",
    "        \"\"\"\n",
    "        determine the constraints of the optimization\n",
    "        \"\"\"\n",
    "\n",
    "        g = x[:n]\n",
    "        h = x[n:2*n]\n",
    "\n",
    "        constraint = []\n",
    "\n",
    "        #constraint.append(C_g @ g + C_h @ h >= 0) # hard mixture monotonicity\n",
    "\n",
    "        constraint.append(cp.diff(g) >= 0)\n",
    "\n",
    "        constraint.append(cp.diff(h) >= 0)\n",
    "\n",
    "        return constraint\n",
    "        \n",
    "    \n",
    "    def penalized_objective(x, n, xs, ys, C_g, C_h):\n",
    "    \n",
    "        \"\"\"\n",
    "        determine the objective function of the optimization\n",
    "        \"\"\"\n",
    "\n",
    "        g = x[:n]\n",
    "        h = x[n:2*n]\n",
    "\n",
    "        xs_g = xs[0] # x values belonging to expert g\n",
    "        ys_g = ys[0] \n",
    "\n",
    "        xs_h = xs[1] # x values belonging to expert h\n",
    "        ys_h = ys[1]\n",
    "\n",
    "        deg = n - 1\n",
    "\n",
    "        alpha = global_alpha \n",
    "        reg1 = alpha * cp.sum_squares(cp.diff(g, 2)) # smoothness\n",
    "        reg2 = alpha * cp.sum_squares(cp.diff(h, 2))\n",
    "\n",
    "        penalty = cp.sum_squares(bernvander(xs_g, deg) @ g - ys_g) + cp.sum_squares(bernvander(xs_h, deg) @ h - ys_h) + reg1 + reg2\n",
    "        penalty += 2 * cp.sum_squares(cp.maximum(0, -(C_g @ g + C_h @ h))) # soft mixture monotonicity\n",
    "        \n",
    "        return penalty\n",
    "    \n",
    "    n = int(len(x0)/2) # num of coefficients\n",
    "    m = len(a) # num of grid points\n",
    "\n",
    "    x0 = cp.Variable(len(x0)) # coefficients from all experts\n",
    "\n",
    "    C_g, C_h = obtain_C(n, a, a_prime, points)\n",
    "    \n",
    "    penalty = penalized_objective(x0, n, xs, ys, C_g, C_h)\n",
    "    constraints = constraint_function(x0, n, C_g, C_h)\n",
    "\n",
    "    problem = cp.Problem(cp.Minimize(penalty), constraints) \n",
    "    problem.solve() \n",
    "    \n",
    "    optimal_g = x0.value[:n]\n",
    "    optimal_h = x0.value[n:2*n]\n",
    "\n",
    "    return optimal_g, optimal_h\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6bfd03-6648-492a-a7d1-866cd03b88ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrated_fitting(z, obsmodel, mixed_X, X=np.linspace(0,1,100)):\n",
    "\n",
    "    \"\"\"\n",
    "    X is the grid points passed to this function\n",
    "    \"\"\"\n",
    "\n",
    "    # creates a new object that is a completely independent copy of the original\n",
    "    z = copy.deepcopy(z)\n",
    "    obsmodel = copy.deepcopy(obsmodel)\n",
    "    mixed_X = copy.deepcopy(mixed_X) # training x values\n",
    "    \n",
    "    unique_z = unique_preserve_order(z[z>=0])\n",
    "\n",
    "    # -------------------- obtain the current g and h --------------\n",
    "    x0 = [] # coefficients for all experts\n",
    "    xs = [] # x values for each cluster\n",
    "    ys = [] # y values for each cluster\n",
    "\n",
    "    for i in np.arange(unique_z.shape[0]): # two \n",
    "\n",
    "        xs.append(obsmodel[unique_z[i]].X.reshape(-1)) # data in each cluster\n",
    "        ys.append(obsmodel[unique_z[i]].Y.reshape(-1))\n",
    "        obsmodel[unique_z[i]].to_fit() # seperate optimization based on data in their own cluster\n",
    "        x0.append(obsmodel[unique_z[i]].u_opt) # the first one is g\n",
    "\n",
    "    x0 = np.concatenate(x0)\n",
    "\n",
    "    # -------------------- calculate the a'(x) ------------------\n",
    "    def k(x, y, gamma=global_gamma):\n",
    "        return np.exp(- gamma * (x - y)**2)\n",
    "    \n",
    "    def dk_dx(x, y, gamma=global_gamma):\n",
    "        return -(x - y) * 2*gamma * k(x, y)\n",
    "        \n",
    "    def a(x, S, T):\n",
    "        numerator = sum(k(x, xi) for xi in S)\n",
    "        denominator = sum(k(x, xj) for xj in T)\n",
    "        return numerator / (denominator + 1e-16)\n",
    "    \n",
    "    def da_dx(x, S, T):\n",
    "        numerator1 = sum(dk_dx(x, xi) for xi in S) * sum(k(x, xj) for xj in T)\n",
    "        numerator2 = sum(k(x, xi) for xi in S) * sum(dk_dx(x, xj) for xj in T)\n",
    "        denominator = (sum(k(x, xj) for xj in T))**2\n",
    "        return (numerator1 - numerator2) / (denominator + 1e-16)\n",
    "\n",
    "    S = mixed_X[z == unique_z[0]].reshape(-1) # the first one is expert g \n",
    "    T = np.concatenate((S, mixed_X[z == unique_z[1]].reshape(-1)))\n",
    "\n",
    "    a_function = np.array([a(x, S, T) for x in X]).reshape(-1) # X is the grid points passed to this function\n",
    "    a_prime = np.array([da_dx(x, S, T) for x in X]).reshape(-1)\n",
    "\n",
    "    # ------------------- update the g and h -------------------\n",
    "    optimal_g, optimal_h = optimize_for_g_and_h_5(a_function, a_prime, X, xs, ys, x0)\n",
    "\n",
    "    obsmodel[unique_z[0]].u_opt = optimal_g # the first one is g\n",
    "    obsmodel[unique_z[1]].u_opt = optimal_h\n",
    "\n",
    "    return obsmodel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7114e9b-4be2-4098-9f97-fb3ba99d4448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_grid_points(z, obsmodel, mixed_X):\n",
    "\n",
    "    grid_points = []\n",
    "\n",
    "    # creates a new object that is a completely independent copy of the original\n",
    "    z = copy.deepcopy(z)\n",
    "    obsmodel = copy.deepcopy(obsmodel)\n",
    "    mixed_X = copy.deepcopy(mixed_X) # training x values\n",
    "\n",
    "    unique_z = unique_preserve_order(z[z>=0])\n",
    "\n",
    "    # -------------------- obtain the current g and h --------------\n",
    "    x0 = [] # coefficients for all experts\n",
    "\n",
    "    for i in np.arange(unique_z.shape[0]): # two \n",
    "        obsmodel[unique_z[i]].to_fit()\n",
    "        x0.append(obsmodel[unique_z[i]].u_opt) \n",
    "\n",
    "    x0 = np.concatenate(x0)\n",
    "\n",
    "    n = int(len(x0)/2) # num of coefficients\n",
    "    g = x0[:n]\n",
    "    h = x0[n:2*n]\n",
    "\n",
    "    # ------------------------------------------------\n",
    "\n",
    "    S = mixed_X[z == unique_z[0]].reshape(-1)\n",
    "    T = np.concatenate((S, mixed_X[z == unique_z[1]].reshape(-1)))\n",
    "\n",
    "    def binomial_prob(n, i, x):\n",
    "        \"\"\"\n",
    "        bionomial PMF for only i and only 1 x\n",
    "        \"\"\"\n",
    "        output = math.comb(n, i) * (x ** i) * ((1 - x) ** (n - i))\n",
    "        return output \n",
    "\n",
    "    def obtain_C(point, n, a, a_prime):\n",
    "\n",
    "        \"\"\"\n",
    "        n is the num of coefficients\n",
    "        a is the weight for expert g at each grid point\n",
    "        \"point\" refers to a single \"grid point\"\n",
    "        \"\"\"\n",
    "\n",
    "        C_g = np.zeros(n)\n",
    "        C_h = np.zeros(n)\n",
    "\n",
    "        b_n_1_head = np.zeros(n) \n",
    "        for i in range(n - 1):\n",
    "            b_n_1_head[i+1] = binomial_prob(n-2, i, point) # zero is at the head\n",
    "\n",
    "        b_n_1_toe = np.zeros(n) \n",
    "        for i in range(n - 1):\n",
    "            b_n_1_toe[i] = binomial_prob(n-2, i, point) # zero is at the toe\n",
    "\n",
    "        b_n = np.zeros(n)\n",
    "        for i in range(n):\n",
    "            b_n[i] = binomial_prob(n-1, i, point)\n",
    "            \n",
    "\n",
    "        C_g = a * b_n_1_head - a * b_n_1_toe + a_prime * b_n \n",
    "        C_h = (1-a) * b_n_1_head - (1-a) * b_n_1_toe - a_prime * b_n \n",
    "        \n",
    "        return C_g, C_h\n",
    "\n",
    "    def k(x, y, gamma=global_gamma):\n",
    "        return np.exp(- gamma * (x - y)**2) \n",
    "    \n",
    "    def dk_dx(x, y, gamma=global_gamma):\n",
    "        return -(x - y) * 2*gamma * k(x, y)\n",
    "    \n",
    "    def a(x, S, T):\n",
    "        numerator = sum(k(x, xi) for xi in S)\n",
    "        denominator = sum(k(x, xj) for xj in T)\n",
    "        return numerator / (denominator + 1e-16)\n",
    "    \n",
    "    def da_dx(x, S, T):\n",
    "        numerator1 = sum(dk_dx(x, xi) for xi in S) * sum(k(x, xj) for xj in T)\n",
    "        numerator2 = sum(k(x, xi) for xi in S) * sum(dk_dx(x, xj) for xj in T)\n",
    "        denominator = (sum(k(x, xj) for xj in T))**2\n",
    "        return (numerator1 - numerator2) / (denominator + 1e-16)\n",
    "\n",
    "    def f_prime(xs, S=S, T=T, n=n, g=g, h=h):\n",
    "        \"\"\"\n",
    "        Provide the function to optimize for \n",
    "        n: the num of coefficients\n",
    "        xs: the grid points\n",
    "        \"\"\"\n",
    "        output = []\n",
    "        for x in xs:\n",
    "            a_function = np.array([a(x, S, T)]).reshape(-1)\n",
    "            a_prime = np.array([da_dx(x, S, T)]).reshape(-1)\n",
    "            C_g, C_h = obtain_C(x, n, a_function, a_prime)\n",
    "            output.append(C_g @ g + C_h @ h)\n",
    "        \n",
    "        return np.array(output)\n",
    "    \n",
    "    critical_points = fsolve(f_prime, np.zeros(1))  # finds the roots of a function, x s.t. f(x)=0\n",
    "    critical_points = critical_points[(critical_points >= 0) & (critical_points <= 1)] # Filter critical points\n",
    "    critical_points = unique_preserve_order(np.sort(critical_points))\n",
    "\n",
    "    # get the intervals between critical points\n",
    "    intervals = [(0, critical_points[0])] if len(critical_points) > 0 else [(0, 1)]\n",
    "    intervals += [(critical_points[i], critical_points[i+1]) for i in range(len(critical_points)-1)]\n",
    "    if len(critical_points) > 0:\n",
    "        intervals.append((critical_points[-1], 1))\n",
    "    \n",
    "    for interval in intervals:\n",
    "        test_point = (interval[0] + interval[1]) / 2 \n",
    "        derivative_value = f_prime([test_point])\n",
    "    \n",
    "        if derivative_value < 0:\n",
    "\n",
    "            temp_n = global_num_grid_points\n",
    "            if interval[0] == interval[1]:\n",
    "                temp_n = 1\n",
    "            \n",
    "            grid_points += list(np.linspace(interval[0], interval[1], temp_n))\n",
    "\n",
    "    if len(grid_points) == 0:\n",
    "        grid_points = np.linspace(0,1,100)\n",
    "\n",
    "    return unique_preserve_order(np.array(grid_points))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6955a553-500d-4a67-9dbb-a666ae9d787d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_variance(z, obsmodel):\n",
    "\n",
    "    # creates a new object that is a completely independent copy of the original\n",
    "    z = copy.deepcopy(z)\n",
    "    obsmodel = copy.deepcopy(obsmodel)\n",
    "\n",
    "    unique_z = unique_preserve_order(z[z>=0])\n",
    "\n",
    "    def binomial_prob(deg, i, x):\n",
    "        \"\"\"\n",
    "        bionomial PMF for only i and only 1 x\n",
    "        \"\"\"\n",
    "        output = comb(deg, i) * (x ** i) * ((1 - x) ** (deg - i))\n",
    "        return output\n",
    "\n",
    "    def obtain_coefficient_var(beta_hat, X, y_data):\n",
    "\n",
    "        \"\"\"\n",
    "        beta_hat is the estimated coefficients\n",
    "        X is the design matrix\n",
    "        y_data is the true response variable\n",
    "        \"\"\"\n",
    "        residuals = y_data - np.dot(X, beta_hat)\n",
    "        \n",
    "        rss = np.sum(residuals**2)\n",
    "        n = len(y_data) # num of training data points\n",
    "        p = len(beta_hat)\n",
    "        sigma2_hat = rss / (n - p) # residual variance, n needs to be > p\n",
    "\n",
    "        matrix = np.dot(X.T, X) + 0.01*np.eye(X.shape[1])\n",
    "        var_beta_hat = sigma2_hat * np.linalg.pinv(matrix) # coefficient uncertainty\n",
    "        # The pseudoinverse is a generalization of the inverse matrix.\n",
    "        # Not all matrices are invertible\n",
    "        # so the pseudoinverse provides a way to obtain a matrix that behaves similarly to an inverse.\n",
    "    \n",
    "        return var_beta_hat, sigma2_hat\n",
    "\n",
    "    for c in np.arange(unique_z.shape[0]): # two\n",
    "    \n",
    "        u = obsmodel[unique_z[c]].u_opt\n",
    "        n = len(u) # num of coefficients\n",
    "        deg = n - 1\n",
    "\n",
    "        x = obsmodel[unique_z[c]].X.reshape(-1)\n",
    "        y = obsmodel[unique_z[c]].Y.reshape(-1)\n",
    "\n",
    "        # design matrix b\n",
    "        b = np.zeros((len(x), n))\n",
    "        for i in np.arange(b.shape[0]):\n",
    "            for k in np.arange(b.shape[1]):\n",
    "                b[i,k] = binomial_prob(deg, k, x[i])\n",
    "        \n",
    "        var, sigma2_hat = obtain_coefficient_var(beta_hat=u, X=b, y_data=y)\n",
    "        obsmodel[unique_z[c]].var = var # coefficient uncertainty\n",
    "        obsmodel[unique_z[c]].noise = sigma2_hat\n",
    "\n",
    "    return obsmodel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6493a6-77b4-4eda-b94f-54ea63a4e94d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MOBP:\n",
    "    \"\"\"\n",
    "    Gibbs sampler, return expected cluster assignments and expert parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, X, Y,\n",
    "                 alpha,\n",
    "                 num_init_clusters=2, \n",
    "                 num_iter=100): \n",
    "\n",
    "        self.X = X  \n",
    "        self.Y = Y  \n",
    " \n",
    "        self.alpha = alpha\n",
    "        self.Nk = None # the number of points in each cluster (including empty clusters)\n",
    "        self.N, _ = X.shape # the number of data points\n",
    "        \n",
    "        self.num_iter = num_iter\n",
    "        self.num_init_clusters = num_init_clusters\n",
    "\n",
    "        self.z = np.zeros(X.shape[0]) # cluster assignments for all the training data points\n",
    "        self.p = dict()  \n",
    "        self.obsmodel = dict()\n",
    "\n",
    "    def initialize_sampler(self, init_K):\n",
    "        \"\"\"\n",
    "        initialize self.z and self.obsmodel\n",
    "        \"\"\"\n",
    "\n",
    "        np.random.seed(0)\n",
    "        x_data = self.X[:, 0].reshape(-1, 1) \n",
    "\n",
    "        initial_labels_ = np.concatenate([np.ones(global_first_cluster_size, dtype=int),\n",
    "                                          np.zeros(len(x_data)-global_first_cluster_size, dtype=int)])\n",
    "        \n",
    "        self.z = initial_labels_\n",
    "        for k in np.arange(len(np.unique(initial_labels_))):\n",
    "            \n",
    "            Xk = self.X[initial_labels_ == k]\n",
    "            Yk = self.Y[initial_labels_ == k]\n",
    "            self.obsmodel[k] = monotonic_poly( \n",
    "                                     Xk[~np.isnan(Xk)].reshape(-1, 1), \n",
    "                                     Yk[~np.isnan(Yk)].reshape(-1, 1))\n",
    "\n",
    "\n",
    "    def sample(self):\n",
    "\n",
    "        self.initialize_sampler(init_K=self.num_init_clusters)\n",
    "\n",
    "        self.Nk = np.bincount(self.z)  # the number of points in each cluster, there could be empty clusters\n",
    "        \n",
    "        idx = np.arange(self.N)\n",
    "        #print('Cluster Initialization: {}'.format(self.Nk[self.Nk > 0]))\n",
    "        \n",
    "        for l in np.arange(1, self.num_iter):\n",
    "            for n in np.random.permutation(idx):\n",
    "\n",
    "                \"\"\"------------------- various updates ------------------------\n",
    "                \"\"\"\n",
    "                \n",
    "                curr_k = self.z[n] # save it, as self.z[n] will be changed\n",
    "                if self.Nk[curr_k] == 1: # skip the point that is alone\n",
    "                    continue\n",
    "\n",
    "                # -------- moved here -----------------------\n",
    "                # find the clusters that satisfy some conditions (only assign to these clusters)\n",
    "                occupancy = np.hstack([self.Nk, self.alpha]) \n",
    "                temp_condition = (occupancy > 0) & (occupancy <= global_ub * self.X.shape[0]) & (occupancy >= global_lb * self.X.shape[0])\n",
    "                active_comp_ids = np.where(temp_condition)[0]\n",
    "                if len(active_comp_ids) > 0:\n",
    "                    active_comp_ids = np.append(active_comp_ids, len(occupancy)-1) # still consider going to a new cluster\n",
    "\n",
    "                if len(active_comp_ids) == 0: # skip this point if no cluster satisfies these conditions\n",
    "                    continue \n",
    "                # -----------------------------------------------\n",
    "\n",
    "                # remove the data point from the current cluster\n",
    "                self.z[n] = -1 \n",
    "                self.Nk[curr_k] -= 1\n",
    "                \n",
    "                if self.X[self.z == curr_k].shape[0] == 0: # if no point after the removal\n",
    "                    self.obsmodel[curr_k] = None\n",
    "                else: # still some points after the removal\n",
    "                    # update the model\n",
    "                    Xk = self.X[self.z == curr_k]\n",
    "                    Yk = self.Y[self.z == curr_k] \n",
    "                    self.obsmodel[curr_k] = monotonic_poly(\n",
    "                                     Xk[~np.isnan(Xk)].reshape(-1, 1), \n",
    "                                     Yk[~np.isnan(Yk)].reshape(-1, 1)) \n",
    "\n",
    "                # create the new model as if the point will go there\n",
    "                new_comp_model = monotonic_poly(\n",
    "                                     self.X[n, ~np.isnan(self.X[n])].reshape(-1, 1), \n",
    "                                     self.Y[n, ~np.isnan(self.Y[n])].reshape(-1, 1))\n",
    "\n",
    "                \"\"\" ------------------------------ end --------------------------------\"\"\"\n",
    "\n",
    "                \"\"\" --------------- calculate the cluster probability ---------------\"\"\"\n",
    "\n",
    "                def kernel(x, y, gamma=global_gamma):\n",
    "                    return np.exp(-gamma * (x - y)**2)\n",
    "\n",
    "                def kernel_2D(x, y, gamma=global_gamma):\n",
    "                    x = x.reshape(-1)\n",
    "                    y = y.reshape(-1)\n",
    "                    \n",
    "                    term1 = -gamma * (x[0] - y[0])**2\n",
    "                    term2 = -gamma/1000 * (x[1] - y[1])**2\n",
    "                    output = np.exp(term1 + term2)\n",
    "                    return output\n",
    "                    \n",
    "                def kernel_derivative(x, y, gamma=global_gamma):\n",
    "                    return -2 * gamma * (x - y) * kernel(x, y, gamma)\n",
    "\n",
    "                def a(x, S, T, gamma=global_gamma):\n",
    "                    numerator = sum(kernel(x, xi, gamma) for xi in S)\n",
    "                    denominator = sum(kernel(x, xi, gamma) for xi in T)\n",
    "                    return numerator / denominator if denominator != 0 else 0\n",
    "\n",
    "                def derivative_a(x, S, T, gamma=global_gamma):\n",
    "                    sum_k_S = np.sum(kernel(x, S, gamma))\n",
    "                    sum_k_T = np.sum(kernel(x, T, gamma))\n",
    "                    sum_k_prime_S = np.sum(kernel_derivative(x, S, gamma))\n",
    "                    sum_k_prime_T = np.sum(kernel_derivative(x, T, gamma))\n",
    "                    numerator = sum_k_prime_S * sum_k_T - sum_k_S * sum_k_prime_T\n",
    "                    denominator = sum_k_T**2\n",
    "                    return numerator / denominator if denominator != 0 else 0\n",
    "\n",
    "                temp_x = self.X[n, ~np.isnan(self.X[n])].reshape(-1, 1) # current point's x value\n",
    "                temp_y = self.Y[n, ~np.isnan(self.Y[n])].reshape(-1, 1) # current point's y value\n",
    "                temp_xy = np.concatenate([temp_x, temp_y], axis=1) # current point\n",
    "\n",
    "                # initialize score, which will help determine which cluster to go to\n",
    "                temppp = 2*np.ones(self.Nk.shape[0], dtype=np.int64) \n",
    "                score = np.hstack([np.log(temppp+1e-16), np.log(self.alpha+1e-16)]) \n",
    "                for i in np.arange(self.Nk.shape[0]):\n",
    "                    \n",
    "                    temp_X = self.X[self.z == i].reshape(-1,1) # the x values in that cluster\n",
    "                    temp_Y = self.Y[self.z == i].reshape(-1,1) # the y values in that cluster\n",
    "                    temp_S = np.concatenate([temp_X, temp_Y], axis=1) # 2D closeness\n",
    "                    #temp_S = temp_X # 1D closeness\n",
    "                    \n",
    "                    #temp_temp = np.sum(kernel(temp_x, temp_S)) # 1D closeness\n",
    "                \n",
    "                    # 2D closeness\n",
    "                    temp_temp = np.array(0.0)\n",
    "                    for temp_s in temp_S: # go through each (x,y) in that cluster\n",
    "                        temp_temp += kernel_2D(temp_xy, temp_s)  # closeness of the point to that cluster\n",
    "                    \n",
    "                    if global_turbine == False:\n",
    "                        score[i] = np.log(temp_temp+1e-16) # replace \n",
    "                    else:\n",
    "                        score[i] = temp_temp \n",
    "                \n",
    "                if global_turbine == False:\n",
    "                    # normalize\n",
    "                    temp_arr = np.array(score)\n",
    "                    temp_bottom = temp_arr.max() - temp_arr.min() \n",
    "                    if temp_bottom < 1e-16:\n",
    "                        temp_bottom = 1e-16\n",
    "                    score = (temp_arr - temp_arr.min()) / temp_bottom\n",
    "                else:\n",
    "                    # alter the score assuming two experts\n",
    "                    temp_arr = np.array(score)[:-1]\n",
    "                    temp_arr_max = temp_arr[np.argmax(temp_arr)]\n",
    "                    temp_arr_min = temp_arr[np.argmin(temp_arr)]\n",
    "                    temp_arr_diff = (temp_arr_max - temp_arr_min) / temp_arr_max # the smaller the percentage, the less of a difference\n",
    "                    score[np.argmax(temp_arr)] = temp_arr_diff\n",
    "                    score[np.argmin(temp_arr)] = 0.0\n",
    "\n",
    "                for i in np.arange(self.Nk.shape[0]): \n",
    "                    score[i] = 5*score[i] # now ranges from 0-5\n",
    "\n",
    "                # -------------------------------- now consider the goodness of fit --------------------------------------\n",
    "                fit_scores = []\n",
    "                # apply our proposed optimization\n",
    "                temp_grid_points = dynamic_grid_points(self.z, self.obsmodel, self.X)\n",
    "                self.obsmodel = integrated_fitting(self.z, self.obsmodel, self.X, X=temp_grid_points)\n",
    "                self.obsmodel = update_variance(self.z, self.obsmodel)\n",
    "                for k in active_comp_ids[:-1]: # only go through the clusters that satisfy the conditions\n",
    "                    \n",
    "                    temp_true = self.Y[n, ~np.isnan(self.Y[n])].reshape(-1, 1) # y value of the current point\n",
    "                    temp_pred = self.obsmodel[k].to_predict(self.X[n, ~np.isnan(self.X[n])].reshape(-1, 1)) # predict for the current point\n",
    "                    temp_score = 1/((((temp_true - temp_pred)** 2).mean())+1e-16) # a measure of the goodness of fit\n",
    "\n",
    "                    if global_turbine == False:\n",
    "                        fit_scores.append(np.log(float(temp_score)+1e-16)) \n",
    "                    else:\n",
    "                        fit_scores.append(float(temp_score))\n",
    "\n",
    "\n",
    "                new_comp = active_comp_ids[-1]\n",
    "                new_comp_model.to_fit()\n",
    "                fit_scores.append(float(np.log(0*new_comp_model.current_score()+1e-16))) # supressed the new expert\n",
    "\n",
    "                if global_turbine == False:\n",
    "                    # noramlize\n",
    "                    temp_arr = np.array(fit_scores)\n",
    "                    fit_scores_updated = ((temp_arr - temp_arr.min()) / (temp_arr.max() - temp_arr.min())).tolist()\n",
    "                else:\n",
    "                    # alter the score assuming two experts\n",
    "                    fit_scores_updated = fit_scores\n",
    "                    temp_arr = np.array(fit_scores_updated)[:-1]\n",
    "                    temp_arr_max = temp_arr[np.argmax(temp_arr)]\n",
    "                    temp_arr_min = temp_arr[np.argmin(temp_arr)]\n",
    "                    temp_arr_diff = (temp_arr_max - temp_arr_min) / temp_arr_max # the smaller the percentage, the less of a difference\n",
    "                    fit_scores_updated[np.argmax(temp_arr)] = temp_arr_diff\n",
    "                    fit_scores_updated[np.argmin(temp_arr)] = 0.0\n",
    "                \n",
    "                temp_count = 0\n",
    "                for k in active_comp_ids: \n",
    "                    score[k] += 5*fit_scores_updated[temp_count]\n",
    "                    temp_count += 1\n",
    "                \n",
    "                final_score = score[active_comp_ids]\n",
    "\n",
    "                # Directly computing sum of exp(a) can lead to numerical issues such as overflow or underflow\n",
    "                # especially when the elements of a are very large or very small.\n",
    "                log_normalizing_constant = lse(final_score) # log of sum of exp(a)\n",
    "                self.p[n] = np.exp(final_score - log_normalizing_constant) # exp(a) / sum of exp(a)\n",
    "\n",
    "                # make the highest value 1 and everybody else 0\n",
    "                temp_probs = np.zeros_like(self.p[n])\n",
    "                temp_index = np.argmax(self.p[n])\n",
    "                temp_probs[temp_index] = 1\n",
    "                                    \n",
    "                \"\"\" ---------------------------------- end ------------------------------- \"\"\"\n",
    "\n",
    "                \"\"\" ---------------------------- updates ----------------------------\n",
    "                \"\"\"\n",
    "                \n",
    "                self.z[n] = npr.choice(active_comp_ids, p=temp_probs) # assign the current data point\n",
    "\n",
    "                if self.z[n] > self.Nk.shape[0]-1: # if going to a new expert\n",
    "                    self.Nk = np.hstack([self.Nk, 1])\n",
    "                else: # if going to an existing expert\n",
    "                    self.Nk[self.z[n]] += 1\n",
    "\n",
    "                if self.z[n] == new_comp: # if going to a new expert\n",
    "                    self.obsmodel[self.z[n]] = new_comp_model # add the new model\n",
    "                else: # if going to an existing model\n",
    "                    self.obsmodel[self.z[n]] = monotonic_poly(\n",
    "                                     np.vstack([self.obsmodel[self.z[n]].X, self.X[n, ~np.isnan(self.X[n])].reshape(-1, 1)]), \n",
    "                                     np.vstack([self.obsmodel[self.z[n]].Y, self.Y[n, ~np.isnan(self.Y[n])].reshape(-1, 1)]))\n",
    "\n",
    "            \"\"\" ------ done looping data ----- \"\"\"\n",
    "            # apply our proposed optimization\n",
    "            temp_grid_points = dynamic_grid_points(self.z, self.obsmodel, self.X)\n",
    "            self.obsmodel = integrated_fitting(self.z, self.obsmodel, self.X, X=temp_grid_points)\n",
    "            self.obsmodel = update_variance(self.z, self.obsmodel)\n",
    "            \n",
    "            # print information\n",
    "            temp_active_comps = self.Nk[self.Nk > 0]\n",
    "            #print('Iter {}: {}'.format(l, temp_active_comps))\n",
    "\n",
    "        \"\"\" ------ done looping iterations ---- \"\"\"\n",
    "\n",
    "#When you load a pickled instance, Python needs to be able to locate the class definition using the module \n",
    "#and class name that were stored when the objectwas pickled.\n",
    "MOGP = MOBP\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c49f79c-6ddd-4775-af61-1227cc7bb6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data():\n",
    "    \"\"\"\n",
    "    generate training data\n",
    "    \"\"\"\n",
    "\n",
    "    choice = global_choice\n",
    "    np.random.seed(42)\n",
    "\n",
    "    if choice == 112.5: # for wavy data\n",
    "\n",
    "        x1 = np.linspace(0.1,0.45001,49)\n",
    "        x1 = np.append(x1, 0.48)\n",
    "        \n",
    "        term = 3*np.pi*1.5*x1\n",
    "        g_x = 1.05*(np.sin(term) + term)/(3*np.pi) + 1.0\n",
    "        y1 = g_x + np.random.normal(0, 0.01, x1.size)\n",
    "\n",
    "        #print(x1.shape)\n",
    "\n",
    "        x2 = np.linspace(0.55727919,0.9,50)\n",
    "        \n",
    "        x_adjust = x2 - x2[0] \n",
    "        y_adjust = 1.780394386232395 \n",
    "        y2 = y_adjust + 2.6e-6*x_adjust + np.random.normal(0, 0.01, x2.size)\n",
    "\n",
    "        #print(x2.shape)\n",
    "\n",
    "        #print('the intersection of the two model is (0.470251, 1.780394)')\n",
    "        #print(f'the middle between the two data clusters is {x1[-1]/2 + x2[0]/2}')\n",
    "\n",
    "        x_combined = np.concatenate([x1, x2]).reshape(-1,1)\n",
    "        y_combined = np.concatenate([y1, y2])\n",
    "    \n",
    "        X = x_combined\n",
    "        Y = y_combined\n",
    "\n",
    "\n",
    "    elif choice == 200: # for material science data\n",
    "        \n",
    "        with open('data/material_sci_data.pkl', 'rb') as f:\n",
    "            mega_summ_stat = pickle.load(f)\n",
    "\n",
    "        summ_stat = mega_summ_stat[0]\n",
    "\n",
    "        CF_xs = summ_stat[0]\n",
    "        CF_ns = summ_stat[1]\n",
    "        CF_means = summ_stat[2]\n",
    "        CF_stds = summ_stat[3]\n",
    "    \n",
    "        DF_xs = summ_stat[4]\n",
    "        DF_ns = summ_stat[5]\n",
    "        DF_means = summ_stat[6]\n",
    "        DF_stds = summ_stat[7]\n",
    "\n",
    "        x1 = CF_xs\n",
    "        x1 = x1[~np.isnan(x1)]\n",
    "        x1 = (x1 - 0) / (70 - 0)\n",
    "        #print(len(x1))\n",
    "        \n",
    "        SE = np.zeros(len(CF_stds))\n",
    "        for i in range(0, len(CF_stds)):\n",
    "            if CF_ns[i] == 0:\n",
    "                SE[i] = np.nan\n",
    "            else:\n",
    "                SE[i] = CF_stds[i] / np.sqrt(CF_ns[i]) \n",
    "\n",
    "        y1 = np.random.normal(CF_means, SE)\n",
    "        y1 = y1[~np.isnan(y1)]\n",
    "\n",
    "        x2 = DF_xs\n",
    "        x2 = x2[~np.isnan(x2)]\n",
    "        x2 = (x2 - 0) / (70 - 0)\n",
    "        #print(len(x2))\n",
    "\n",
    "        SE = np.zeros(len(DF_stds))\n",
    "        for i in range(0, len(DF_stds)):\n",
    "            if DF_ns[i] == 0:\n",
    "                SE[i] = np.nan\n",
    "            else:\n",
    "                SE[i] = DF_stds[i] / np.sqrt(DF_ns[i])\n",
    "\n",
    "        y2 = np.random.normal(DF_means, SE)\n",
    "        y2 = y2[~np.isnan(y2)]\n",
    "        \n",
    "        x_combined = np.concatenate([x1, x2]).reshape(-1,1)\n",
    "        y_combined = np.concatenate([y1, y2])\n",
    "    \n",
    "        X = x_combined\n",
    "        X = add_turbulence_new(X.reshape(-1).copy()).reshape(-1,1)\n",
    "        Y = y_combined\n",
    "    \n",
    "    elif choice == 304.555: # for turbine data\n",
    "        \n",
    "        df = pd.read_pickle('data/turbine_train_data.pkl')\n",
    "        \n",
    "        x = np.array(df.iloc[:,0])\n",
    "        x = (x - 3.5) / (19.9 - 3.5) # normalize\n",
    "        y = np.array(df.iloc[:,1])\n",
    "        \n",
    "        X = x.reshape(-1,1)\n",
    "        X = add_turbulence(X.reshape(-1).copy()).reshape(-1,1)\n",
    "        Y = y\n",
    "        \n",
    "    #print(X.shape, Y.shape)\n",
    "\n",
    "    # Plot the data for visualization\n",
    "    # plt.figure(figsize=(10, 6))\n",
    "    # plt.scatter(X, Y, s=10)\n",
    "    # plt.title('Generated Data')\n",
    "    # plt.xlabel('X')\n",
    "    # plt.ylabel('Y')\n",
    "    # plt.grid(True)\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f2794a-2ec2-4561-8b6a-319e68ef20a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test(X, Y, c_test):\n",
    "\n",
    "    \"\"\"\n",
    "    to generate test data\n",
    "    note: do not use random_state of 42\n",
    "    \"\"\"\n",
    "    random_state = 1\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    if c_test == 1112: # testing data for wavy \n",
    "\n",
    "        X_test = []\n",
    "        Y_test = []\n",
    "        \n",
    "        for test_i in range(0,30):\n",
    "            \n",
    "            true_threshold = 0.470251\n",
    "            x1 = np.zeros(25)\n",
    "            even_x1 = np.sort(np.random.uniform(0.1,true_threshold-0.0001,x1.size)) \n",
    "\n",
    "            np.random.normal(0, 0.01, x1.size) # for reproducibility\n",
    "            \n",
    "            term = 3*np.pi*1.5*even_x1 \n",
    "            g_x = 1.05*(np.sin(term) + term)/(3*np.pi) + 1.0\n",
    "            y1 = g_x + np.random.normal(0, 0.01, even_x1.size)\n",
    "        \n",
    "            x2 = np.zeros(25)\n",
    "            even_x2 = np.sort(np.random.uniform(true_threshold+0.0001,0.9,x2.size))\n",
    "            \n",
    "            x_adjust = even_x2 - x2[0]\n",
    "            y_adjust = 1.780394386232395 \n",
    "            y2 = y_adjust + 2.6e-6*x_adjust + np.random.normal(0, 0.01, even_x2.size)\n",
    "            \n",
    "            x_combined = np.concatenate([even_x1, even_x2]).reshape(-1,1)\n",
    "            y_combined = np.concatenate([y1, y2])\n",
    "        \n",
    "            X_test_current = x_combined\n",
    "            Y_test_current = y_combined\n",
    "\n",
    "            X_test.append(X_test_current)\n",
    "            Y_test.append(Y_test_current)\n",
    "\n",
    "    elif c_test == 1200: # testing data for material science\n",
    "\n",
    "        X_test = []\n",
    "        Y_test = []\n",
    "\n",
    "        for test_i in range(0,30):\n",
    "        \n",
    "            with open('data/material_sci_data.pkl', 'rb') as f:\n",
    "                mega_summ_stat = pickle.load(f)\n",
    "    \n",
    "            summ_stat = mega_summ_stat[0]\n",
    "    \n",
    "            CF_xs = summ_stat[0]\n",
    "            CF_ns = summ_stat[1]\n",
    "            CF_means = summ_stat[2]\n",
    "            CF_stds = summ_stat[3]\n",
    "        \n",
    "            DF_xs = summ_stat[4]\n",
    "            DF_ns = summ_stat[5]\n",
    "            DF_means = summ_stat[6]\n",
    "            DF_stds = summ_stat[7]\n",
    "    \n",
    "            x1 = CF_xs\n",
    "            x1 = x1[~np.isnan(x1)]\n",
    "            x1 = (x1 - 0) / (70 - 0) + np.random.normal(0,0.0001,x1.size)\n",
    "            x1[x1 < 0] = 0\n",
    "            x1[x1 > 1] = 1\n",
    "            #print(len(x1))\n",
    "            \n",
    "            SE = np.zeros(len(CF_stds))\n",
    "            for i in range(0, len(CF_stds)):\n",
    "                if CF_ns[i] == 0:\n",
    "                    SE[i] = np.nan\n",
    "                else:\n",
    "                    SE[i] = CF_stds[i] / np.sqrt(CF_ns[i]) \n",
    "    \n",
    "            y1 = np.random.normal(CF_means, SE)\n",
    "            y1 = y1[~np.isnan(y1)]\n",
    "    \n",
    "            x2 = DF_xs\n",
    "            x2 = x2[~np.isnan(x2)]\n",
    "            x2 = (x2 - 0) / (70 - 0) + np.random.normal(0,0.0001,x2.size)\n",
    "            x2[x2 < 0] = 0\n",
    "            x2[x2 > 1] = 1\n",
    "            #print(len(x2))\n",
    "    \n",
    "            SE = np.zeros(len(DF_stds))\n",
    "            for i in range(0, len(DF_stds)):\n",
    "                if DF_ns[i] == 0:\n",
    "                    SE[i] = np.nan\n",
    "                else:\n",
    "                    SE[i] = DF_stds[i] / np.sqrt(DF_ns[i])\n",
    "    \n",
    "            y2 = np.random.normal(DF_means, SE)\n",
    "            y2 = y2[~np.isnan(y2)]\n",
    "            \n",
    "            x_combined = np.concatenate([x1, x2]).reshape(-1,1)\n",
    "            y_combined = np.concatenate([y1, y2])\n",
    "        \n",
    "            X_test_current = x_combined\n",
    "            X_test_current = add_turbulence_new(X_test_current.reshape(-1).copy()).reshape(-1,1)\n",
    "            Y_test_current = y_combined\n",
    "\n",
    "            indices = np.random.choice(len(X_test_current), size=10, replace=False)\n",
    "\n",
    "            X_test.append(X_test_current[indices])\n",
    "            Y_test.append(Y_test_current[indices])\n",
    "\n",
    "    elif c_test == 1304: # filtered testing data for turbine\n",
    "\n",
    "        with open(\"data/turbine_test_data_filtered.pkl\", \"rb\") as f:\n",
    "            # Load the pickled list from the file\n",
    "            df_list = pickle.load(f)\n",
    "\n",
    "        X_test = []\n",
    "        Y_test = []\n",
    "\n",
    "        for df in df_list:\n",
    "\n",
    "            df = df[df.iloc[:, 0] <= 19.9]\n",
    "            \n",
    "            x = np.array(df.iloc[:,0])\n",
    "            x = (x - 3.5) / (19.9 - 3.5) # normalize in the same way\n",
    "            y = np.array(df.iloc[:,1])\n",
    "            \n",
    "            X_test_current = x.reshape(-1,1)\n",
    "            X_test_current = add_turbulence(X_test_current.reshape(-1).copy()).reshape(-1,1)\n",
    "            Y_test_current = y\n",
    "\n",
    "            X_test.append(X_test_current)\n",
    "            Y_test.append(Y_test_current)\n",
    "\n",
    "    elif c_test == 1304.1: # unfiltered testing data for turbine\n",
    "\n",
    "        with open(\"data/turbine_test_data_unfiltered.pkl\", \"rb\") as f:\n",
    "            # Load the pickled list from the file\n",
    "            df_list = pickle.load(f)\n",
    "\n",
    "        X_test = []\n",
    "        Y_test = []\n",
    "\n",
    "        for df in df_list:\n",
    "\n",
    "            df = df[df.iloc[:, 0] <= 19.9]\n",
    "            \n",
    "            x = np.array(df.iloc[:,0])\n",
    "            x = (x - 3.5) / (19.9 - 3.5) # normalize in the same way\n",
    "            y = np.array(df.iloc[:,1])\n",
    "            \n",
    "            X_test_current = x.reshape(-1,1)\n",
    "            X_test_current = add_turbulence(X_test_current.reshape(-1).copy()).reshape(-1,1)\n",
    "            Y_test_current = y\n",
    "\n",
    "            X_test.append(X_test_current)\n",
    "            Y_test.append(Y_test_current)\n",
    "\n",
    "    X_train = X\n",
    "    Y_train = Y\n",
    "\n",
    "    # Plot the data for visualization\n",
    "    # plt.figure(figsize=(10, 6))\n",
    "    # if isinstance(X_test, list) != True:\n",
    "    #     plt.scatter(X_test, Y_test, s=10)\n",
    "    # else:\n",
    "    #     plt.scatter(X_test[0], Y_test[0], s=10)\n",
    "    # plt.title('Generated Data')\n",
    "    # plt.xlabel('X')\n",
    "    # plt.ylabel('Y')\n",
    "    # plt.grid(True)\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "    return X_train, X_test, Y_train, Y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa93092d-3d4e-4478-bfd9-3a51f6db04a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantifiable_results(y_true, y_pred, x, show_mono=True):\n",
    "\n",
    "    x = x.reshape(-1) # test x \n",
    "    y_pred = np.array(y_pred) # predicted y value for the test x\n",
    "\n",
    "    if y_true is not None: # the true y value for the test x\n",
    "        y_true = np.array(y_true)\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        mae = None\n",
    "        r2 = None\n",
    "    else:\n",
    "        mse = None\n",
    "        mae = None\n",
    "        r2 = None\n",
    "        \n",
    "    y_pred_rounded = np.round(y_pred, decimals=5)\n",
    "    correlation, _ = spearmanr(x, y_pred_rounded)\n",
    "    monotonicity_score = correlation\n",
    "    if show_mono == True:\n",
    "        print(f\"Monotonicity Score: {monotonicity_score}\")\n",
    "\n",
    "    return mse, mae, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c87524-6e89-4777-b9ce-b901c9d1337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_result2(mixing, mixed_X, mixed_Y, clusters_X=None, clusters_Y=None, mixed_X_test=None, mixed_Y_test=None, show_mono=True):\n",
    "\n",
    "    \"\"\"\n",
    "    mixed_X_test, mixed_Y_test: optional testing data to predict for, otherwise we are using testing values across the domain\n",
    "    \n",
    "    \"\"\"\n",
    "    X = np.linspace(0,1,1000).reshape(-1,1) # the x values across the domain to predict for\n",
    "    Y_true = None\n",
    "    \n",
    "    if mixed_Y_test is not None:\n",
    "        temp_index = np.argsort(mixed_X_test.reshape(-1)) \n",
    "        X = mixed_X_test[temp_index].reshape(-1,1) # sorted testing x values\n",
    "        Y_true = mixed_Y_test[temp_index] # sorted testing y values\n",
    "    \n",
    "    z = mixing.z\n",
    "    unique_z = unique_preserve_order(z)\n",
    "\n",
    "    # clustering metrics\n",
    "    # data_2D = np.concatenate((mixed_X, mixed_Y.reshape(-1,1)), axis=1)\n",
    "    # dbi = davies_bouldin_score(data_2D, mixing.z)\n",
    "    # print(f'Davies-Bouldin Index: {dbi:.4f}')\n",
    "    # true_labels = np.concatenate([np.ones(42, dtype=int),np.zeros(44, dtype=int)])\n",
    "    # ari_metric = adjusted_rand_score(true_labels, mixing.z)\n",
    "    # print(f'Adjusted Rand Index (ARI): {ari_metric:.4f}')\n",
    "    # accuracy = accuracy_score(true_labels, mixing.z)\n",
    "    # print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    means_matrix = [] # pred means from all models\n",
    "    vars_matrix = [] # pred variances from all models\n",
    "    for i in np.arange(unique_z.shape[0]):\n",
    "        \n",
    "        mean = mixing.obsmodel[unique_z[i]].to_predict(X)\n",
    "        means_matrix.append(mean.reshape(-1))\n",
    "        var = mixing.obsmodel[unique_z[i]].to_predict_var(X)\n",
    "        var = np.diag(var)\n",
    "        vars_matrix.append(var.reshape(-1))\n",
    "    \n",
    "    def kernel(x, y, gamma=global_gamma):\n",
    "        return np.exp(-gamma * (x - y)**2)\n",
    "    \n",
    "    weights_matrix = [] # the closeness of all test points to all clusters, # test points by # clusters\n",
    "    for x in X:\n",
    "        raw_weights = [] # the closeness of current test point to all clusters\n",
    "        for i in np.arange(unique_z.shape[0]):\n",
    "            current_set = mixed_X[mixing.z == unique_z[i],:].reshape(-1) # the x values from this cluster\n",
    "            temp_weight1 = np.sum(kernel(x, current_set)) # the closeness of current test point to the cluster\n",
    "            raw_weights.append(temp_weight1)\n",
    "\n",
    "        normalizing_constant = np.sum(raw_weights) \n",
    "        weights = raw_weights / normalizing_constant\n",
    "        \n",
    "        weights_matrix.append(weights)\n",
    "\n",
    "    weighted_means = [] # mixture prediction\n",
    "    weighted_vars = []\n",
    "    for i in range(len(X)): \n",
    "        \n",
    "        weights = weights_matrix[i] # the closeness of current test point to all clusters\n",
    "        weights_vars = [item**2 for item in weights] \n",
    "\n",
    "        model_means = [model[i] for model in means_matrix] # prediction for this test point from each model\n",
    "        model_vars = [model[i] for model in vars_matrix] \n",
    "\n",
    "        weighted_means.append(float(sum([a * b for a, b in zip(model_means, weights)]))) \n",
    "        weighted_vars.append(float(sum([a * b for a, b in zip(model_vars, weights_vars)])))  \n",
    "    \n",
    "    actual_y = weighted_means # mixture prediction\n",
    "\n",
    "    mse, mae, r2 = quantifiable_results(Y_true, actual_y, X, show_mono=show_mono)\n",
    "\n",
    "    weighted_stds = [np.sqrt(item) for item in weighted_vars]\n",
    "\n",
    "    lbs = [weighted_means[i] - 1.96 * weighted_stds[i] for i in range(len(X))]\n",
    "    ubs = [weighted_means[i] + 1.96 * weighted_stds[i] for i in range(len(X))]\n",
    "\n",
    "    return [X, actual_y, lbs, ubs, Y_true, mse, mae, r2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ce4697-75bd-42a4-8f4d-a1ce66d2b9ee",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_result(z, mixed_X, mixed_Y, clusters_X=None, clusters_Y=None, option=\"both + mixture_pred\",\n",
    "                     plot_mixture_pred=None, case_study=\"wavy\", save_file=False):\n",
    "\n",
    "    \"\"\"\n",
    "    mixed_X and mixed_Y are the training data \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    unique_z = unique_preserve_order(z) \n",
    "\n",
    "    means = [] # predictions for all sorted training x values, from each model\n",
    "    vars = []\n",
    "    sub_means = [] # predictions for sorted training x values from each cluster, from corresponding model\n",
    "    sub_vars = []\n",
    "\n",
    "    sub_Xs = []  # list of training x values from each cluster (sorted)\n",
    "    sub_Xs_original = [] # list of training x values from each cluster (without sorting)\n",
    "    sub_Ys_original = []\n",
    "\n",
    "    entire_X = np.sort(mixed_X.reshape(-1,1), axis=0) # sorted training x values (from all clusters)\n",
    "    entire_X_original = mixed_X.reshape(-1,1) # without sorting\n",
    "    entire_Y_original = mixed_Y.reshape(-1,1)\n",
    "\n",
    "    for i in np.arange(unique_z.shape[0]):\n",
    "        \n",
    "        index = np.where(z == unique_z[i])[0]\n",
    "\n",
    "        # all the training points of a particular cluster\n",
    "        X = np.sort(mixed_X[index].reshape(-1,1), axis=0) # sorted x values from this cluster\n",
    "        X_original = mixed_X[index].reshape(-1,1) # without sorting\n",
    "        Y_original = mixed_Y[index].reshape(-1,1) # without sorting\n",
    "        sub_Xs_original.append(X_original)\n",
    "        sub_Ys_original.append(Y_original)\n",
    "        \n",
    "        if X[~np.isnan(X)].shape[0] > 0:\n",
    "            \n",
    "            mean = mixing.obsmodel[unique_z[i]].to_predict(entire_X)\n",
    "            means.append(mean)\n",
    "            \n",
    "            sub_Xs.append(X)\n",
    "            sub_mean = mixing.obsmodel[unique_z[i]].to_predict(X)\n",
    "            sub_means.append(sub_mean)\n",
    "            \n",
    "            sub_var = mixing.obsmodel[unique_z[i]].to_predict_var(X)\n",
    "            sub_vars.append(np.diag(sub_var))\n",
    "\n",
    "\n",
    "    if plot_mixture_pred is not None:\n",
    "\n",
    "        plt.close()\n",
    "        colors = ['#ADD8E6','#FFC87C'] \n",
    "        shapes = ['o', 's']\n",
    "        linestyles = ['--','-.']\n",
    "        \n",
    "        plt.style.use('default')\n",
    "        plt.figure(figsize=(10, 6), dpi=300)\n",
    "        \n",
    "        plt.xlabel('X', fontsize=22, labelpad=10) \n",
    "        plt.ylabel('Y', fontsize=22, labelpad=10) \n",
    "        plt.xticks(fontsize=20) \n",
    "        plt.yticks(fontsize=20) \n",
    "\n",
    "        # extract from the input\n",
    "        mixture_X = plot_mixture_pred[0]\n",
    "        mixture_Y = plot_mixture_pred[1]\n",
    "        lbs = plot_mixture_pred[2]\n",
    "        ubs = plot_mixture_pred[3]\n",
    "        mixture_Y_true = plot_mixture_pred[4]\n",
    "\n",
    "        if case_study == \"turbine\":\n",
    "            mixture_X = mixture_X*(19.9-3.5)+3.5\n",
    "            sub_Xs_original = [item*(19.9-3.5)+3.5 for item in sub_Xs_original]\n",
    "\n",
    "        # Plot each data subset with distinct colors and markers\n",
    "        for i in np.arange(unique_z.shape[0]):\n",
    "            i_replace = i\n",
    "            if case_study == \"material_sci\":\n",
    "                i_replace = i\n",
    "            plt.scatter(\n",
    "                sub_Xs_original[i_replace], sub_Ys_original[i_replace],\n",
    "                label=f'Assigned data for expert {i+1}', \n",
    "                color=colors[i],  \n",
    "                marker=shapes[i],\n",
    "                alpha=0.85,  \n",
    "                s=60  \n",
    "            )\n",
    "        \n",
    "        # Plot the mean prediction line\n",
    "        plt.plot(\n",
    "            mixture_X, mixture_Y,\n",
    "            label='Mixture Prediction', color='black',\n",
    "            linewidth=2.5, linestyle='-', alpha=0.9\n",
    "        )\n",
    "\n",
    "        # Plotting the prediction interval \n",
    "        plt.fill_between(mixture_X[:, 0], \n",
    "                         lbs, \n",
    "                         ubs, \n",
    "                         alpha=0.5,  \n",
    "                         color='lightgrey', \n",
    "                         label='95% Mixture Prediction Interval')  \n",
    "        \n",
    "        # Optionally, add dashed lines to highlight the bounds of the interval\n",
    "        # *(19.9-3.5)+3.5\n",
    "        plt.plot(mixture_X[:, 0], lbs, color='gray', linestyle='--', linewidth=1.5)  # Lower bound\n",
    "        plt.plot(mixture_X[:, 0], ubs, color='gray', linestyle='--', linewidth=1.5)  # Upper bound\n",
    "        \n",
    "        if case_study == \"material_sci\":\n",
    "            leg_loc = 'upper left'\n",
    "        else:\n",
    "            leg_loc = 'lower right'\n",
    "        \n",
    "        plt.legend(\n",
    "            loc=leg_loc, borderaxespad=1, # the padding between the legends border and the axes\n",
    "            prop={'size': 19},  \n",
    "            markerscale=1.5, # scales the marker size in the legend relative to their original size in the plot\n",
    "            frameon=True, \n",
    "            framealpha=0.8,  \n",
    "            facecolor='white',  \n",
    "            edgecolor='grey'   \n",
    "        )\n",
    "        \n",
    "        plt.tight_layout() # for minimal white space\n",
    "\n",
    "        if case_study == \"wavy\":\n",
    "            plt.ylim(1.1,2.01)\n",
    "            plt.xlim(0.09,0.91)\n",
    "\n",
    "        if case_study == \"material_sci\":\n",
    "            plt.ylim(-15,55)\n",
    "            plt.xlim(0,0.95)\n",
    "\n",
    "        if case_study == \"turbine\":\n",
    "            plt.ylim(-0.3,1.3)\n",
    "            plt.xlim(3.5,19.3)\n",
    "\n",
    "        if save_file == True:\n",
    "            plt.savefig('publication_figures/'+case_study+'_proposed.png', dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b38f005-9bbb-4d0a-8875-b304e51da91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_mixture(mixed_X, mixed_Y, mixed_X_test=None, mixed_Y_test=None, plot_mixture_pred=None, show_mono=True, case_study=\"wavy\", save_file=False, gpr_model=None):\n",
    "\n",
    "    \"\"\"\n",
    "    mixed_X, mixed_Y: required training data\n",
    "    mixed_X_test, mixed_Y_test: optional testing data to predict for, otherwise we are using testing values across the domain\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    test_x = np.linspace(0,1,1000).reshape(-1, 1) # the x values across the domain to predict for\n",
    "    Y_true = None\n",
    "\n",
    "    if mixed_X_test is not None:\n",
    "        temp_index = np.argsort(mixed_X_test.reshape(-1)) \n",
    "        test_x= mixed_X_test[temp_index].reshape(-1,1) # sorted testing x values\n",
    "        Y_true = mixed_Y_test[temp_index] # sorted testing y values\n",
    "\n",
    "\n",
    "    # def standard_GPR(training_x, training_y, testing_x):\n",
    "\n",
    "    #     # hyperparameter optimization\n",
    "    #     # (initial value for the first run, it is allowed to vary between the range)\n",
    "    #     if case_study == \"material_sci\":\n",
    "    #         kernel = C(1.0, (1e-4, 5000)) * RBF(1.0, (1e-4, 10)) + WhiteKernel(1.0, (1e-5, 10))\n",
    "    #     else:\n",
    "    #         kernel = C(1.0, (1e-4, 10)) * RBF(1.0, (1e-4, 10)) + WhiteKernel(1.0, (1e-5, 10))\n",
    "    #     gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)\n",
    "    #     # restart the hyperparameter optimization 10 times with different initial guesses\n",
    "\n",
    "    #     # fit\n",
    "    #     temp_start = time.time()\n",
    "    #     gpr.fit(training_x, training_y)\n",
    "    #     temp_end = time.time()\n",
    "    #     if plot_mixture_pred is not None:\n",
    "    #         print(f'time took: {temp_end - temp_start} seconds')\n",
    "\n",
    "    #     # predict\n",
    "    #     y_pred, sigma = gpr.predict(testing_x, return_std=True)\n",
    "\n",
    "    #     lbs = y_pred - 1.96 * sigma\n",
    "    #     ubs = y_pred + 1.96 * sigma\n",
    "\n",
    "    #     return y_pred, lbs, ubs\n",
    "        \n",
    "    # pred_y, lbs, ubs = standard_GPR(training_x=mixed_X, training_y=mixed_Y, testing_x=test_x)\n",
    "\n",
    "    def standard_GPR(training_x, training_y):\n",
    "\n",
    "        # hyperparameter optimization\n",
    "        # (initial value for the first run, it is allowed to vary between the range)\n",
    "        if case_study == \"material_sci\":\n",
    "            kernel = C(1.0, (1e-4, 5000)) * RBF(1.0, (1e-4, 10)) + WhiteKernel(1.0, (1e-5, 10))\n",
    "        else:\n",
    "            kernel = C(1.0, (1e-4, 10)) * RBF(1.0, (1e-4, 10)) + WhiteKernel(1.0, (1e-5, 10))\n",
    "        gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)\n",
    "        # restart the hyperparameter optimization 10 times with different initial guesses\n",
    "\n",
    "        # fit\n",
    "        temp_start = time.time()\n",
    "        gpr.fit(training_x, training_y)\n",
    "        temp_end = time.time()\n",
    "        if plot_mixture_pred is not None:\n",
    "            print(f'time took: {temp_end - temp_start} seconds')\n",
    "\n",
    "        return gpr\n",
    "\n",
    "    def GPR_pred(gpr, testing_x):\n",
    "\n",
    "        # predict\n",
    "        y_pred, sigma = gpr.predict(testing_x, return_std=True)\n",
    "\n",
    "        lbs = y_pred - 1.96 * sigma\n",
    "        ubs = y_pred + 1.96 * sigma\n",
    "\n",
    "        return y_pred, lbs, ubs\n",
    "\n",
    "    if plot_mixture_pred is not None:\n",
    "        gpr_model = standard_GPR(training_x=mixed_X, training_y=mixed_Y)\n",
    "    \n",
    "    pred_y, lbs, ubs = GPR_pred(gpr=gpr_model, testing_x=test_x)\n",
    "\n",
    "    mse, mae, r2 = quantifiable_results(Y_true, pred_y, test_x, show_mono=show_mono)\n",
    "\n",
    "    # ------------------------------------------------------ plot -------------------------------------\n",
    "    if plot_mixture_pred is not None:\n",
    "        colors = ['#ADD8E6','#FFC87C']\n",
    "        plt.style.use('default')\n",
    "        plt.figure(figsize=(10, 6), dpi=300)\n",
    "        \n",
    "        # save: r\"Normalized $\\text{Stress} * \\text{Volume}^{1/6}$\"\n",
    "        plt.xlabel('X', fontsize=22, labelpad=10) \n",
    "        plt.ylabel('Y', fontsize=22, labelpad=10)\n",
    "        \n",
    "        plt.xticks(fontsize=20)\n",
    "        plt.yticks(fontsize=20)\n",
    "\n",
    "        if case_study == \"turbine\":\n",
    "            mixed_X = mixed_X*(19.9-3.5)+3.5\n",
    "            test_x= test_x*(19.9-3.5)+3.5\n",
    "    \n",
    "        # plot the training data\n",
    "        plt.scatter(\n",
    "            mixed_X, mixed_Y,\n",
    "            label=f'Data',\n",
    "            color=colors[0],  \n",
    "            alpha=0.85,  \n",
    "            s=60\n",
    "        )\n",
    "    \n",
    "        # plot the predictions\n",
    "        plt.plot(\n",
    "            test_x, pred_y,\n",
    "            label='Prediction', color='black',\n",
    "            linewidth=2.5, linestyle='-', alpha=0.9\n",
    "        )\n",
    "    \n",
    "        # plot the prediction interval\n",
    "        if lbs is not None and ubs is not None:\n",
    "            plt.fill_between(test_x.reshape(-1), \n",
    "                             lbs, \n",
    "                             ubs, \n",
    "                             alpha=0.5,  \n",
    "                             color='lightgrey', \n",
    "                             label='95% Prediction Interval')  \n",
    "    \n",
    "        # highlight the lower and upper bounds of interval\n",
    "        # *(19.9-3.5)+3.5\n",
    "        plt.plot(test_x.reshape(-1), lbs, color='gray', linestyle='--', linewidth=1.5) \n",
    "        plt.plot(test_x.reshape(-1), ubs, color='gray', linestyle='--', linewidth=1.5)  \n",
    "\n",
    "        if case_study == \"material_sci\":\n",
    "            leg_loc = 'upper left'\n",
    "        else:\n",
    "            leg_loc = 'lower right'\n",
    "        \n",
    "        plt.legend(\n",
    "            loc=leg_loc, borderaxespad=1, # the padding between the legends border and the axes\n",
    "            prop={'size': 19}, \n",
    "            markerscale=1.5, # scales the marker size in the legend relative to their original size in the plot\n",
    "            frameon=True, \n",
    "            framealpha=0.8,  \n",
    "            facecolor='white',  \n",
    "            edgecolor='grey'   \n",
    "        )\n",
    "        \n",
    "        plt.tight_layout() # for minimal white space\n",
    "    \n",
    "        if case_study == \"wavy\":\n",
    "            plt.ylim(1.1,2.01)\n",
    "            plt.xlim(0.09,0.91)\n",
    "\n",
    "        if case_study == \"material_sci\":\n",
    "            plt.ylim(-15,55)\n",
    "            plt.xlim(0,0.95)\n",
    "\n",
    "        if case_study == \"turbine\":\n",
    "            plt.ylim(-0.3,1.3)\n",
    "            plt.xlim(3.5,19.3)\n",
    "\n",
    "        if save_file == True:\n",
    "            plt.savefig('publication_figures/'+case_study+'_GP.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "        plt.show()\n",
    "\n",
    "    return mse, mae, r2, gpr_model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f5d06c-3601-4b8c-9f44-dbb45e1a0ee8",
   "metadata": {},
   "source": [
    "## Function Calls and Results\n",
    "*Choose the dataset to see the corresponding figure, monotonicity score, average MSE, and standard error of MSE*\n",
    "- *Dataset choices: ```\"wavy\"``` for synthetic data, ```\"material_sci\"```, and ```\"turbine\"```*\n",
    "- *Results will appear in the output, set ```save_file``` to be ```True``` if wishing to save the figures to the ```publication_figures/``` folder*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bae49e-127a-4e8c-9fec-7952f26e787f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure\n",
    "case_study = \"material_sci\"\n",
    "save_file = False\n",
    "\n",
    "if case_study == \"material_sci\":\n",
    "    global_gamma = 2000\n",
    "    global_choice = 200\n",
    "    global_c_test = 1200\n",
    "    global_first_cluster_size = 44\n",
    "    global_degree = 20\n",
    "    global_alpha = 2\n",
    "    global_num_iter = 5\n",
    "    global_ub = 0.6\n",
    "    global_lb = 0.4\n",
    "    global_turbine = False\n",
    "    global_num_grid_points = 5\n",
    "elif case_study == \"wavy\":\n",
    "    global_gamma = 200#5000\n",
    "    global_choice = 112.5\n",
    "    global_c_test = 1112\n",
    "    global_first_cluster_size = 50\n",
    "    global_degree = 40\n",
    "    global_alpha = 0.5\n",
    "    global_num_iter = 5\n",
    "    global_ub = 0.7\n",
    "    global_lb = 0.4\n",
    "    global_turbine = False\n",
    "    global_num_grid_points = 15#5\n",
    "elif case_study == \"turbine\":\n",
    "    global_gamma = 5000\n",
    "    global_choice = 304.555\n",
    "    global_c_test_filtered = 1304\n",
    "    global_c_test_unfiltered = 1304.1\n",
    "    global_first_cluster_size = 425\n",
    "    global_degree = 15\n",
    "    global_alpha = 2\n",
    "    global_num_iter = 20\n",
    "    global_ub = 0.7\n",
    "    global_lb = 0.3\n",
    "    global_turbine = True\n",
    "    global_num_grid_points = 5\n",
    "\n",
    "# supress some warnings\n",
    "warnings.resetwarnings()\n",
    "warnings.filterwarnings(\"ignore\",\n",
    "                        category=RuntimeWarning,\n",
    "                        module=\"scipy.optimize._minpack_py\" \n",
    ")\n",
    "warnings.filterwarnings(\"ignore\",\n",
    "                        category=ConvergenceWarning,\n",
    "                        module=\"sklearn.gaussian_process.kernels\"\n",
    ")\n",
    "\n",
    "# generate training data\n",
    "mixed_X, mixed_Y = generate_data()\n",
    "# generate testing data\n",
    "if case_study == \"turbine\":\n",
    "    mixed_X_train, mixed_X_test_filtered, mixed_Y_train, mixed_Y_test_filtered = generate_test(mixed_X, mixed_Y, global_c_test_filtered)\n",
    "    _, mixed_X_test_unfiltered, __, mixed_Y_test_unfiltered = generate_test(mixed_X, mixed_Y, global_c_test_unfiltered)\n",
    "else:\n",
    "    mixed_X_train, mixed_X_test, mixed_Y_train, mixed_Y_test = generate_test(mixed_X, mixed_Y, global_c_test)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b268636c-7e87-4489-8d84-732b081daf02",
   "metadata": {},
   "source": [
    "### 1. Proposed MMoE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1f5bb1-5806-4681-91e0-001996b61f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the mixture model\n",
    "mixing = MOBP(X=mixed_X_train, Y=mixed_Y_train, alpha=0, num_init_clusters=2, num_iter=global_num_iter)\n",
    "# fit the mixture model\n",
    "if case_study != \"turbine\":\n",
    "    temp_start = time.time()\n",
    "    mixing.sample()\n",
    "    temp_end = time.time()\n",
    "    print(f'time took: {temp_end - temp_start} seconds')\n",
    "else:\n",
    "    with open(\"proposed_fitted_models/mixing_turbine.pkl\", \"rb\") as f:\n",
    "        mixing = pickle.load(f)\n",
    "\n",
    "# generate what is needed for figure\n",
    "plot_items = visualize_result2(mixing, mixed_X_train, mixed_Y_train, mixed_X_test=None, mixed_Y_test=None)\n",
    "\n",
    "# generate the metrics\n",
    "if case_study == \"turbine\":\n",
    "    total_num_seeds = len(mixed_X_test_filtered)\n",
    "    \n",
    "    mse_list = []\n",
    "    for test_i in range(0, total_num_seeds):\n",
    "        plot_items_ = visualize_result2(mixing, mixed_X_train, mixed_Y_train, mixed_X_test=mixed_X_test_filtered[test_i], mixed_Y_test=mixed_Y_test_filtered[test_i], show_mono=False)\n",
    "        mse_list.append(plot_items_[5])\n",
    "\n",
    "    print('For filtered test set:')\n",
    "    print(f'average mse over {total_num_seeds} seeds is {sum(mse_list)/total_num_seeds}')\n",
    "    print(f'SE of mse is {np.std(mse_list)/np.sqrt(total_num_seeds)}')\n",
    "\n",
    "    # repeat\n",
    "    mse_list = []\n",
    "    for test_i in range(0, total_num_seeds):\n",
    "        plot_items_ = visualize_result2(mixing, mixed_X_train, mixed_Y_train, mixed_X_test=mixed_X_test_unfiltered[test_i], mixed_Y_test=mixed_Y_test_unfiltered[test_i], show_mono=False)\n",
    "        mse_list.append(plot_items_[5])\n",
    "\n",
    "    print('For unfiltered test set:')\n",
    "    print(f'average mse over {total_num_seeds} seeds is {sum(mse_list)/total_num_seeds}')\n",
    "    print(f'SE of mse is {np.std(mse_list)/np.sqrt(total_num_seeds)}')\n",
    "else:\n",
    "    total_num_seeds = len(mixed_X_test)\n",
    "    \n",
    "    mse_list = []\n",
    "    for test_i in range(0, total_num_seeds):\n",
    "        plot_items_ = visualize_result2(mixing, mixed_X_train, mixed_Y_train, mixed_X_test=mixed_X_test[test_i], mixed_Y_test=mixed_Y_test[test_i], show_mono=False)\n",
    "        mse_list.append(plot_items_[5])\n",
    "        \n",
    "    print(f'average mse over {total_num_seeds} seeds is {sum(mse_list)/total_num_seeds}')\n",
    "    print(f'SE of mse is {np.std(mse_list)/np.sqrt(total_num_seeds)}')\n",
    "\n",
    "\n",
    "\n",
    "# generate the figure\n",
    "visualize_result(mixing.z, mixed_X_train, mixed_Y_train, option='both + mixture_pred', plot_mixture_pred=plot_items, case_study=case_study, save_file=save_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcd360d-6d2a-48ad-bb5d-542b065e989a",
   "metadata": {},
   "source": [
    "### 2. Standard GPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9321d0-9785-4af2-93ea-40b186a2a8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the figure for GP\n",
    "_, _, _, gpr_model = no_mixture(mixed_X=mixed_X_train, mixed_Y=mixed_Y_train, mixed_X_test=None, mixed_Y_test=None, plot_mixture_pred=True, case_study=case_study, save_file=save_file)\n",
    "\n",
    "# generate the metrics for GP\n",
    "\n",
    "if case_study == \"turbine\":\n",
    "\n",
    "    total_num_seeds = len(mixed_X_test_filtered)\n",
    "    \n",
    "    mse_list = []\n",
    "    for test_i in range(0, total_num_seeds):\n",
    "        metrics_pred = no_mixture(mixed_X=mixed_X_train, mixed_Y=mixed_Y_train, mixed_X_test=mixed_X_test_filtered[test_i], mixed_Y_test=mixed_Y_test_filtered[test_i], show_mono=False, case_study=case_study, gpr_model=gpr_model)\n",
    "        mse_list.append(metrics_pred[0])\n",
    "\n",
    "    print('For filtered test set:')\n",
    "    print(f'average mse over {total_num_seeds} seeds is {sum(mse_list)/total_num_seeds}')\n",
    "    print(f'SE of mse is {np.std(mse_list)/np.sqrt(total_num_seeds)}')\n",
    "\n",
    "    # repeat\n",
    "    mse_list = []\n",
    "    for test_i in range(0, total_num_seeds):\n",
    "        metrics_pred = no_mixture(mixed_X=mixed_X_train, mixed_Y=mixed_Y_train, mixed_X_test=mixed_X_test_unfiltered[test_i], mixed_Y_test=mixed_Y_test_unfiltered[test_i], show_mono=False, case_study=case_study, gpr_model=gpr_model)\n",
    "        mse_list.append(metrics_pred[0])\n",
    "\n",
    "    print('For unfiltered test set:')\n",
    "    print(f'average mse over {total_num_seeds} seeds is {sum(mse_list)/total_num_seeds}')\n",
    "    print(f'SE of mse is {np.std(mse_list)/np.sqrt(total_num_seeds)}')\n",
    "        \n",
    "else:\n",
    "    total_num_seeds = len(mixed_X_test)\n",
    "    \n",
    "    mse_list = []\n",
    "    for test_i in range(0, total_num_seeds):\n",
    "        metrics_pred = no_mixture(mixed_X=mixed_X_train, mixed_Y=mixed_Y_train, mixed_X_test=mixed_X_test[test_i], mixed_Y_test=mixed_Y_test[test_i], show_mono=False, case_study=case_study, gpr_model=gpr_model)\n",
    "        mse_list.append(metrics_pred[0])\n",
    "    \n",
    "    print(f'average mse over {total_num_seeds} seeds is {sum(mse_list)/total_num_seeds}')\n",
    "    print(f'SE of mse is {np.std(mse_list)/np.sqrt(total_num_seeds)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaedca4-5d93-4f84-a511-2338a678cf3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
